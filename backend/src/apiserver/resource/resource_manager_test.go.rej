diff a/backend/src/apiserver/resource/resource_manager_test.go b/backend/src/apiserver/resource/resource_manager_test.go	(rejected hunks)
@@ -15,12 +15,14 @@
 package resource
 
 import (
+	"context"
 	"fmt"
 	"strings"
 	"testing"
 	"time"
 
-	"github.com/argoproj/argo/pkg/apis/workflow/v1alpha1"
+	"github.com/argoproj/argo-workflows/v3/pkg/apis/workflow/v1alpha1"
+	"github.com/golang/protobuf/ptypes/timestamp"
 	api "github.com/kubeflow/pipelines/backend/api/go_client"
 	"github.com/kubeflow/pipelines/backend/src/apiserver/client"
 	"github.com/kubeflow/pipelines/backend/src/apiserver/common"
@@ -33,6 +35,7 @@ import (
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/require"
 	"google.golang.org/grpc/codes"
+	corev1 "k8s.io/api/core/v1"
 	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/types"
 )
@@ -70,8 +73,18 @@ func (m *FakeBadObjectStore) GetFromYamlFile(o interface{}, filePath string) err
 var testWorkflow = util.NewWorkflow(&v1alpha1.Workflow{
 	TypeMeta:   v1.TypeMeta{APIVersion: "argoproj.io/v1alpha1", Kind: "Workflow"},
 	ObjectMeta: v1.ObjectMeta{Name: "workflow-name", UID: "workflow1", Namespace: "ns1"},
-	Spec:       v1alpha1.WorkflowSpec{Arguments: v1alpha1.Arguments{Parameters: []v1alpha1.Parameter{{Name: "param1"}}}},
-	Status:     v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeRunning},
+	Spec: v1alpha1.WorkflowSpec{
+		Entrypoint: "testy",
+		Templates: []v1alpha1.Template{v1alpha1.Template{
+			Name: "testy",
+			Container: &corev1.Container{
+				Image:   "docker/whalesay",
+				Command: []string{"cowsay"},
+				Args:    []string{"hello world"},
+			},
+		}},
+		Arguments: v1alpha1.Arguments{Parameters: []v1alpha1.Parameter{{Name: "param1"}}}},
+	Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowRunning},
 })
 
 // Util function to create an initial state with pipeline uploaded
@@ -165,7 +220,7 @@ func initWithPatchedRun(t *testing.T) (*FakeClientManager, *ResourceManager, *mo
 			},
 		},
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	runDetail, err := manager.CreateRun(context.Background(), apiRun)
 	assert.Nil(t, err)
 	return store, manager, runDetail
 }
@@ -187,13 +242,14 @@ func initWithOneTimeFailedRun(t *testing.T) (*FakeClientManager, *ResourceManage
 			},
 		},
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	ctx := context.Background()
+	runDetail, err := manager.CreateRun(ctx, apiRun)
 	assert.Nil(t, err)
 	updatedWorkflow := util.NewWorkflow(testWorkflow.DeepCopy())
 	updatedWorkflow.SetLabels(util.LabelKeyWorkflowRunId, runDetail.UUID)
-	updatedWorkflow.Status.Phase = v1alpha1.NodeFailed
+	updatedWorkflow.Status.Phase = v1alpha1.WorkflowFailed
 	updatedWorkflow.Status.Nodes = map[string]v1alpha1.NodeStatus{"node1": {Name: "pod1", Type: v1alpha1.NodeTypePod, Phase: v1alpha1.NodeFailed}}
-	err = manager.ReportWorkflowResource(updatedWorkflow)
+	err = manager.ReportWorkflowResource(ctx, updatedWorkflow)
 	assert.Nil(t, err)
 	return store, manager, runDetail
 }
@@ -230,6 +286,40 @@ func TestCreatePipeline(t *testing.T) {
 	assert.Equal(t, pipelineExpected, pipeline)
 }
 
+func TestCreatePipeline_V2PipelineName(t *testing.T) {
+	tests := []struct {
+		name         string
+		namespace    string
+		pipelineName string
+	}{
+		{name: "v2-compat", namespace: "", pipelineName: "pipeline/v2-compat"},
+		{name: "pipe3", namespace: "", pipelineName: "pipeline/pipe3"},
+		{name: "pipeline2", namespace: "kubeflow", pipelineName: "namespace/kubeflow/pipeline/pipeline2"},
+		{name: "abcd", namespace: "user", pipelineName: "namespace/user/pipeline/abcd"},
+	}
+	for _, test := range tests {
+		t.Run(fmt.Sprintf("%+v", test), func(t *testing.T) {
+			store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
+			defer store.Close()
+			manager := NewResourceManager(store)
+
+			createdPipeline, err := manager.CreatePipeline(test.name, "", test.namespace, []byte(strings.TrimSpace(
+				v2compatPipeline)))
+			require.Nil(t, err)
+			params, err := util.UnmarshalParameters(createdPipeline.Parameters)
+			require.Nil(t, err)
+			var nameParam *v1alpha1.Parameter
+			for _, param := range params {
+				if param.Name == "pipeline-name" {
+					nameParam = &param
+				}
+			}
+			require.NotNil(t, nameParam)
+			require.Equal(t, test.pipelineName, nameParam.Value.String())
+		})
+	}
+}
+
 func TestCreatePipeline_ComplexPipeline(t *testing.T) {
 	store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
 	defer store.Close()
@@ -242,13 +332,13 @@ func TestCreatePipeline_ComplexPipeline(t *testing.T) {
 	assert.Nil(t, err)
 }
 
-func TestCreatePipeline_GetParametersError(t *testing.T) {
+func TestCreatePipeline_ParseWorkflowError(t *testing.T) {
 	store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
 	defer store.Close()
 	manager := NewResourceManager(store)
 	_, err := manager.CreatePipeline("pipeline1", "", "", []byte("I am invalid yaml"))
 	assert.Equal(t, codes.InvalidArgument, err.(*util.UserError).ExternalStatusCode())
-	assert.Contains(t, err.Error(), "Failed to parse the parameter")
+	assert.Contains(t, err.Error(), "Failed to parse the workflow template")
 }
 
 func TestCreatePipeline_StorePipelineMetadataError(t *testing.T) {
@@ -347,16 +437,20 @@ func TestCreateRun_ThroughPipelineID(t *testing.T) {
 			},
 		},
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	runDetail, err := manager.CreateRun(context.Background(), apiRun)
 	assert.Nil(t, err)
 
 	expectedRuntimeWorkflow := testWorkflow.DeepCopy()
-	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{
-		{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
+	AddRuntimeMetadata(expectedRuntimeWorkflow)
 	expectedRuntimeWorkflow.Labels = map[string]string{util.LabelKeyWorkflowRunId: "123e4567-e89b-12d3-a456-426655440000"}
 	expectedRuntimeWorkflow.Annotations = map[string]string{util.AnnotationKeyRunName: "run1"}
+	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
 	expectedRuntimeWorkflow.Spec.ServiceAccountName = defaultPipelineRunnerServiceAccount
-
+	expectedRuntimeWorkflow.Spec.PodMetadata = &v1alpha1.Metadata{
+		Labels: map[string]string{
+			util.LabelKeyWorkflowRunId: DefaultFakeUUID,
+		},
+	}
 	expectedRunDetail := &model.RunDetail{
 		Run: model.Run{
 			UUID:           "123e4567-e89b-12d3-a456-426655440000",
@@ -408,11 +502,17 @@ func TestCreateRun_ThroughWorkflowSpec(t *testing.T) {
 	store, manager, runDetail := initWithOneTimeRun(t)
 	expectedExperimentUUID := runDetail.ExperimentUUID
 	expectedRuntimeWorkflow := testWorkflow.DeepCopy()
-	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{
-		{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
+	AddRuntimeMetadata(expectedRuntimeWorkflow)
 	expectedRuntimeWorkflow.Labels = map[string]string{util.LabelKeyWorkflowRunId: "123e4567-e89b-12d3-a456-426655440000"}
 	expectedRuntimeWorkflow.Annotations = map[string]string{util.AnnotationKeyRunName: "run1"}
+	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
 	expectedRuntimeWorkflow.Spec.ServiceAccountName = defaultPipelineRunnerServiceAccount
+	expectedRuntimeWorkflow.Spec.PodMetadata = &v1alpha1.Metadata{
+		Labels: map[string]string{
+			util.LabelKeyWorkflowRunId: DefaultFakeUUID,
+		},
+	}
+
 	expectedRunDetail := &model.RunDetail{
 		Run: model.Run{
 			UUID:           "123e4567-e89b-12d3-a456-426655440000",
@@ -457,11 +557,17 @@ func TestCreateRun_ThroughWorkflowSpecWithPatch(t *testing.T) {
 	store, manager, runDetail := initWithPatchedRun(t)
 	expectedExperimentUUID := runDetail.ExperimentUUID
 	expectedRuntimeWorkflow := testWorkflow.DeepCopy()
-	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{
-		{Name: "param1", Value: v1alpha1.AnyStringPtr("test-default-bucket")}}
+	AddRuntimeMetadata(expectedRuntimeWorkflow)
 	expectedRuntimeWorkflow.Labels = map[string]string{util.LabelKeyWorkflowRunId: "123e4567-e89b-12d3-a456-426655440000"}
 	expectedRuntimeWorkflow.Annotations = map[string]string{util.AnnotationKeyRunName: "run1"}
+	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{{Name: "param1", Value: v1alpha1.AnyStringPtr("test-default-bucket")}}
 	expectedRuntimeWorkflow.Spec.ServiceAccountName = defaultPipelineRunnerServiceAccount
+	expectedRuntimeWorkflow.Spec.PodMetadata = &v1alpha1.Metadata{
+		Labels: map[string]string{
+			util.LabelKeyWorkflowRunId: DefaultFakeUUID,
+		},
+	}
+
 	expectedRunDetail := &model.RunDetail{
 		Run: model.Run{
 			UUID:           "123e4567-e89b-12d3-a456-426655440000",
@@ -539,15 +645,20 @@ func TestCreateRun_ThroughPipelineVersion(t *testing.T) {
 		},
 		ServiceAccount: "sa1",
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	runDetail, err := manager.CreateRun(context.Background(), apiRun)
 	assert.Nil(t, err)
 
 	expectedRuntimeWorkflow := testWorkflow.DeepCopy()
-	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{
-		{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
+	AddRuntimeMetadata(expectedRuntimeWorkflow)
 	expectedRuntimeWorkflow.Labels = map[string]string{util.LabelKeyWorkflowRunId: "123e4567-e89b-12d3-a456-426655440000"}
 	expectedRuntimeWorkflow.Annotations = map[string]string{util.AnnotationKeyRunName: "run1"}
+	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
 	expectedRuntimeWorkflow.Spec.ServiceAccountName = "sa1"
+	expectedRuntimeWorkflow.Spec.PodMetadata = &v1alpha1.Metadata{
+		Labels: map[string]string{
+			util.LabelKeyWorkflowRunId: DefaultFakeUUID,
+		},
+	}
 
 	expectedRunDetail := &model.RunDetail{
 		Run: model.Run{
@@ -635,15 +746,20 @@ func TestCreateRun_ThroughPipelineIdAndPipelineVersion(t *testing.T) {
 		},
 		ServiceAccount: "sa1",
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	runDetail, err := manager.CreateRun(context.Background(), apiRun)
 	assert.Nil(t, err)
 
 	expectedRuntimeWorkflow := testWorkflow.DeepCopy()
-	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{
-		{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
+	AddRuntimeMetadata(expectedRuntimeWorkflow)
 	expectedRuntimeWorkflow.Labels = map[string]string{util.LabelKeyWorkflowRunId: "123e4567-e89b-12d3-a456-426655440000"}
 	expectedRuntimeWorkflow.Annotations = map[string]string{util.AnnotationKeyRunName: "run1"}
+	expectedRuntimeWorkflow.Spec.Arguments.Parameters = []v1alpha1.Parameter{{Name: "param1", Value: v1alpha1.AnyStringPtr("world")}}
 	expectedRuntimeWorkflow.Spec.ServiceAccountName = "sa1"
+	expectedRuntimeWorkflow.Spec.PodMetadata = &v1alpha1.Metadata{
+		Labels: map[string]string{
+			util.LabelKeyWorkflowRunId: DefaultFakeUUID,
+		},
+	}
 
 	expectedRunDetail := &model.RunDetail{
 		Run: model.Run{
@@ -711,7 +827,7 @@ func TestCreateRun_NoExperiment(t *testing.T) {
 		// No experiment
 		ResourceReferences: []*api.ResourceReference{},
 	}
-	runDetail, err := manager.CreateRun(apiRun)
+	runDetail, err := manager.CreateRun(context.Background(), apiRun)
 	assert.Nil(t, err)
 	expectedRunDetail := []*model.ResourceReference{{
 		ResourceUUID: "123e4567-e89b-12d3-a456-426655440000",
@@ -776,7 +892,7 @@ func TestCreateRun_NullWorkflowSpec(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateRun(apiRun)
+	_, err := manager.CreateRun(context.Background(), apiRun)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to fetch workflow spec manifest.: ResourceNotFoundError: WorkflowSpecManifest run1 not found.")
 }
@@ -794,7 +910,7 @@ func TestCreateRun_OverrideParametersError(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateRun(apiRun)
+	_, err := manager.CreateRun(context.Background(), apiRun)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Unrecognized input parameter")
 }
@@ -813,7 +929,7 @@ func TestCreateRun_CreateWorkflowError(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateRun(apiRun)
+	_, err := manager.CreateRun(context.Background(), apiRun)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to create a workflow")
 }
@@ -832,7 +948,7 @@ func TestCreateRun_StoreRunMetadataError(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateRun(apiRun)
+	_, err := manager.CreateRun(context.Background(), apiRun)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "database is closed")
 }
@@ -840,7 +956,7 @@ func TestCreateRun_StoreRunMetadataError(t *testing.T) {
 func TestDeleteRun(t *testing.T) {
 	store, manager, runDetail := initWithOneTimeRun(t)
 	defer store.Close()
-	err := manager.DeleteRun(runDetail.UUID)
+	err := manager.DeleteRun(context.Background(), runDetail.UUID)
 	assert.Nil(t, err)
 
 	_, err = manager.GetRun(runDetail.UUID)
@@ -862,7 +978,7 @@ func TestDeleteRun_CrdFailure(t *testing.T) {
 	defer store.Close()
 
 	manager.argoClient = client.NewFakeArgoClientWithBadWorkflow()
-	err := manager.DeleteRun(runDetail.UUID)
+	err := manager.DeleteRun(context.Background(), runDetail.UUID)
 	//assert.Equal(t, codes.Internal, err.(*util.UserError).ExternalStatusCode())
 	//assert.Contains(t, err.Error(), "some error")
 	// TODO(IronPan) This should return error if swf CRD doesn't cascade delete runs.
@@ -874,7 +990,7 @@ func TestDeleteRun_DbFailure(t *testing.T) {
 	defer store.Close()
 
 	store.DB().Close()
-	err := manager.DeleteRun(runDetail.UUID)
+	err := manager.DeleteRun(context.Background(), runDetail.UUID)
 	assert.Equal(t, codes.Internal, err.(*util.UserError).ExternalStatusCode())
 	assert.Contains(t, err.Error(), "database is closed")
 }
@@ -946,7 +1062,7 @@ func TestTerminateRun(t *testing.T) {
 	store, manager, runDetail := initWithOneTimeRun(t)
 	defer store.Close()
 
-	err := manager.TerminateRun(runDetail.UUID)
+	err := manager.TerminateRun(context.Background(), runDetail.UUID)
 	assert.Nil(t, err)
 
 	actualRunDetail, err := manager.GetRun(runDetail.UUID)
@@ -972,7 +1088,7 @@ func TestTerminateRun_DbFailure(t *testing.T) {
 	defer store.Close()
 
 	store.DB().Close()
-	err := manager.TerminateRun(runDetail.UUID)
+	err := manager.TerminateRun(context.Background(), runDetail.UUID)
 	assert.Equal(t, codes.Internal, err.(*util.UserError).ExternalStatusCode())
 	assert.Contains(t, err.Error(), "database is closed")
 }
@@ -985,7 +1101,7 @@ func TestRetryRun(t *testing.T) {
 	assert.Nil(t, err)
 	assert.Contains(t, actualRunDetail.WorkflowRuntimeManifest, "Failed")
 
-	err = manager.RetryRun(runDetail.UUID)
+	err = manager.RetryRun(context.Background(), runDetail.UUID)
 	assert.Nil(t, err)
 
 	actualRunDetail, err = manager.GetRun(runDetail.UUID)
@@ -1007,7 +1123,7 @@ func TestRetryRun_FailedDeletePods(t *testing.T) {
 	defer store.Close()
 
 	manager.k8sCoreClient = client.NewFakeKubernetesCoreClientWithBadPodClient()
-	err := manager.RetryRun(runDetail.UUID)
+	err := manager.RetryRun(context.Background(), runDetail.UUID)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "failed to delete pod")
 }
@@ -1017,7 +1133,7 @@ func TestRetryRun_UpdateAndCreateFailed(t *testing.T) {
 	defer store.Close()
 
 	manager.argoClient = client.NewFakeArgoClientWithBadWorkflow()
-	err := manager.RetryRun(runDetail.UUID)
+	err := manager.RetryRun(context.Background(), runDetail.UUID)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to create or update the run")
 }
@@ -1094,7 +1210,7 @@ func TestCreateJob_ThroughPipelineID(t *testing.T) {
 
 	// The pipeline specified via pipeline id will be converted to this
 	// pipeline's default version, which will be used to create run.
-	newJob, err := manager.CreateJob(job)
+	newJob, err := manager.CreateJob(context.Background(), job)
 	expectedJob := &model.Job{
 		UUID:           "123e4567-e89b-12d3-a456-426655440000",
 		DisplayName:    "j1",
@@ -1174,7 +1290,7 @@ func TestCreateJob_ThroughPipelineVersion(t *testing.T) {
 			},
 		},
 	}
-	newJob, err := manager.CreateJob(job)
+	newJob, err := manager.CreateJob(context.Background(), job)
 	expectedJob := &model.Job{
 		UUID:           "123e4567-e89b-12d3-a456-426655440000",
 		DisplayName:    "j1",
@@ -1253,7 +1369,7 @@ func TestCreateJob_ThroughPipelineIdAndPipelineVersion(t *testing.T) {
 			},
 		},
 	}
-	newJob, err := manager.CreateJob(job)
+	newJob, err := manager.CreateJob(context.Background(), job)
 	expectedJob := &model.Job{
 		UUID:           "123e4567-e89b-12d3-a456-426655440000",
 		DisplayName:    "j1",
@@ -1306,7 +1422,7 @@ func TestCreateJob_EmptyPipelineSpec(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateJob(job)
+	_, err := manager.CreateJob(context.Background(), job)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to fetch workflow spec")
 }
@@ -1325,7 +1441,7 @@ func TestCreateJob_InvalidWorkflowSpec(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateJob(job)
+	_, err := manager.CreateJob(context.Background(), job)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to unmarshal workflow spec manifest")
 }
@@ -1344,7 +1460,7 @@ func TestCreateJob_NullWorkflowSpec(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateJob(job)
+	_, err := manager.CreateJob(context.Background(), job)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Failed to fetch workflow spec manifest.: ResourceNotFoundError: WorkflowSpecManifest pp 1 not found.")
 }
@@ -1362,7 +1478,7 @@ func TestCreateJob_ExtraInputParameterError(t *testing.T) {
 			},
 		},
 	}
-	_, err := manager.CreateJob(job)
+	_, err := manager.CreateJob(context.Background(), job)
 	assert.Equal(t, codes.InvalidArgument, err.(*util.UserError).ExternalStatusCode())
 	assert.Contains(t, err.Error(), "Unrecognized input parameter: param2")
 }
@@ -1376,7 +1492,7 @@ func TestCreateJob_FailedToCreateScheduleWorkflow(t *testing.T) {
 		Enabled:      true,
 		PipelineSpec: &api.PipelineSpec{PipelineId: p.UUID},
 	}
-	_, err := manager.CreateJob(job)
+	_, err := manager.CreateJob(context.Background(), job)
 	assert.Equal(t, codes.Internal, err.(*util.UserError).ExternalStatusCode())
 	assert.Contains(t, err.Error(), "Failed to create a scheduled workflow")
 }
@@ -1384,7 +1500,7 @@ func TestCreateJob_FailedToCreateScheduleWorkflow(t *testing.T) {
 func TestEnableJob(t *testing.T) {
 	store, manager, job := initWithJob(t)
 	defer store.Close()
-	err := manager.EnableJob(job.UUID, false)
+	err := manager.EnableJob(context.Background(), job.UUID, false)
 	job, err = manager.GetJob(job.UUID)
 	expectedJob := &model.Job{
 		UUID:           "123e4567-e89b-12d3-a456-426655440000",
@@ -1437,9 +1553,9 @@ func TestEnableJob_CustomResourceNotFound(t *testing.T) {
 	defer store.Close()
 	// The swf CR can be missing when user reinstalled KFP using existing DB data.
 	// Explicitly delete it to simulate the situation.
-	manager.getScheduledWorkflowClient(job.Namespace).Delete(job.Name, &v1.DeleteOptions{})
+	manager.getScheduledWorkflowClient(job.Namespace).Delete(context.Background(), job.Name, &v1.DeleteOptions{})
 	// When swf CR is missing, enabling the job needs to fail.
-	err := manager.EnableJob(job.UUID, true)
+	err := manager.EnableJob(context.Background(), job.UUID, true)
 	assert.Equal(t, codes.Internal, err.(*util.UserError).ExternalStatusCode())
 	assert.Contains(t, err.Error(), "Check job exist failed")
 	assert.Contains(t, err.Error(), "not found")
@@ -1452,8 +1568,8 @@ func TestDisableJob_CustomResourceNotFound(t *testing.T) {
 
 	// The swf CR can be missing when user reinstalled KFP using existing DB data.
 	// Explicitly delete it to simulate the situation.
-	manager.getScheduledWorkflowClient(job.Namespace).Delete(job.Name, &v1.DeleteOptions{})
-	err := manager.EnableJob(job.UUID, false)
+	manager.getScheduledWorkflowClient(job.Namespace).Delete(context.Background(), job.Name, &v1.DeleteOptions{})
+	err := manager.EnableJob(context.Background(), job.UUID, false)
 	require.Nil(t, err, "Disabling the job should succeed even when the custom resource is missing.")
 	job, err = manager.GetJob(job.UUID)
 	require.Nil(t, err)
@@ -1504,10 +1620,10 @@ func TestDeleteJob_CustomResourceNotFound(t *testing.T) {
 	defer store.Close()
 	// The swf CR can be missing when user reinstalled KFP using existing DB data.
 	// Explicitly delete it to simulate the situation.
-	manager.getScheduledWorkflowClient(job.Namespace).Delete(job.Name, &v1.DeleteOptions{})
+	manager.getScheduledWorkflowClient(job.Namespace).Delete(context.Background(), job.Name, &v1.DeleteOptions{})
 
 	// Now deleting job should still succeed when the swf CR is already deleted.
-	err := manager.DeleteJob(job.UUID)
+	err := manager.DeleteJob(context.Background(), job.UUID)
 	assert.Nil(t, err)
 
 	// And verify Job has been deleted from DB too.
@@ -1538,9 +1654,9 @@ func TestReportWorkflowResource_ScheduledWorkflowIDEmpty_Success(t *testing.T) {
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID},
 			Namespace: "ns1",
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeRunning},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowRunning},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 	runDetail, err := manager.GetRun(run.UUID)
 	assert.Nil(t, err)
@@ -1592,7 +1708,7 @@ func TestReportWorkflowResource_ScheduledWorkflowIDNotEmpty_Success(t *testing.T
 			CreationTimestamp: v1.NewTime(time.Unix(11, 0).UTC()),
 		},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 
 	runDetail, err := manager.GetRun("WORKFLOW_1")
@@ -1647,7 +1763,7 @@ func TestReportWorkflowResource_ScheduledWorkflowIDNotEmpty_NoExperiment_Success
 		PipelineSpec: &api.PipelineSpec{WorkflowManifest: testWorkflow.ToStringForStore()},
 		// no experiment reference
 	}
-	newJob, err := manager.CreateJob(job)
+	newJob, err := manager.CreateJob(context.Background(), job)
 
 	// report workflow
 	workflow := util.NewWorkflow(&v1alpha1.Workflow{
@@ -1666,7 +1782,7 @@ func TestReportWorkflowResource_ScheduledWorkflowIDNotEmpty_NoExperiment_Success
 		},
 	})
 
-	err = manager.ReportWorkflowResource(workflow)
+	err = manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 
 	runDetail, err := manager.GetRun("WORKFLOW_1")
@@ -1719,11 +1835,30 @@ func TestReportWorkflowResource_WorkflowMissingRunID(t *testing.T) {
 			Name: run.Name,
 		},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "Workflow[workflow-name] missing the Run ID label")
 }
 
+func TestReportWorkflowResource_RunNotFound(t *testing.T) {
+	store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
+	manager := NewResourceManager(store)
+	ctx := context.Background()
+	defer store.Close()
+	workflow := util.NewWorkflow(&v1alpha1.Workflow{
+		ObjectMeta: v1.ObjectMeta{
+			Name:      "obsolete",
+			Namespace: "kubeflow",
+			Labels:    map[string]string{util.LabelKeyWorkflowRunId: "run-id-not-exist"},
+		},
+	})
+	store.ArgoClient().Workflow("kubeflow").Create(ctx, workflow.Workflow, v1.CreateOptions{})
+	err := manager.ReportWorkflowResource(ctx, workflow)
+	require.NotNil(t, err)
+	assert.True(t, util.IsUserErrorCodeMatch(err, codes.NotFound))
+	assert.Contains(t, err.Error(), "Run run-id-not-exist not found")
+}
+
 func TestReportWorkflowResource_WorkflowCompleted(t *testing.T) {
 	store, manager, run := initWithOneTimeRun(t)
 	namespace := "kubeflow"
@@ -1736,12 +1871,12 @@ func TestReportWorkflowResource_WorkflowCompleted(t *testing.T) {
 			UID:       types.UID(run.UUID),
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID},
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeFailed},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowFailed},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 
-	wf, err := store.ArgoClientFake.Workflow(namespace).Get(run.Run.Name, v1.GetOptions{})
+	wf, err := store.ArgoClientFake.Workflow(namespace).Get(context.Background(), run.Run.Name, v1.GetOptions{})
 	assert.Nil(t, err)
 	assert.Equal(t, wf.Labels[util.LabelKeyWorkflowPersistedFinalState], "true")
 }
@@ -1756,9 +1891,9 @@ func TestReportWorkflowResource_WorkflowCompleted_WorkflowNotFound(t *testing.T)
 			UID:       types.UID(run.UUID),
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID},
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeFailed},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowFailed},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	require.NotNil(t, err)
 	assert.Equalf(t, codes.NotFound, err.(*util.UserError).ExternalStatusCode(), "Expected not found error, but got %s", err.Error())
 	assert.Contains(t, err.Error(), "Failed to add PersistedFinalState label")
@@ -1775,9 +1910,9 @@ func TestReportWorkflowResource_WorkflowCompleted_FinalStatePersisted(t *testing
 			UID:       types.UID(run.UUID),
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID, util.LabelKeyWorkflowPersistedFinalState: "true"},
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeFailed},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowFailed},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 }
 
@@ -1791,9 +1926,9 @@ func TestReportWorkflowResource_WorkflowCompleted_FinalStatePersisted_WorkflowNo
 			UID:       types.UID(run.UUID),
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID, util.LabelKeyWorkflowPersistedFinalState: "true"},
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeFailed},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowFailed},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	require.NotNil(t, err)
 	assert.Equalf(t, codes.NotFound, err.(*util.UserError).ExternalStatusCode(), "Expected not found error, but got %s", err.Error())
 	assert.Contains(t, err.Error(), "Failed to delete the completed workflow")
@@ -1811,9 +1946,9 @@ func TestReportWorkflowResource_WorkflowCompleted_FinalStatePersisted_DeleteFail
 			UID:       types.UID(run.UUID),
 			Labels:    map[string]string{util.LabelKeyWorkflowRunId: run.UUID, util.LabelKeyWorkflowPersistedFinalState: "true"},
 		},
-		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.NodeFailed},
+		Status: v1alpha1.WorkflowStatus{Phase: v1alpha1.WorkflowFailed},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.NotNil(t, err)
 	assert.Contains(t, err.Error(), "failed to delete workflow")
 }
@@ -1889,7 +2024,7 @@ func TestReportScheduledWorkflowResource_Error(t *testing.T) {
 		Enabled:      true,
 		PipelineSpec: &api.PipelineSpec{PipelineId: p.UUID},
 	}
-	newJob, err := manager.CreateJob(job)
+	newJob, err := manager.CreateJob(context.Background(), job)
 	assert.Nil(t, err)
 
 	store.Close()
@@ -1986,7 +2121,7 @@ func TestReadArtifact_Succeed(t *testing.T) {
 			},
 		},
 	})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 
 	artifactContent, err := manager.ReadArtifact("run-1", "node-1", "artifact-1")
@@ -2012,7 +2147,7 @@ func TestReadArtifact_WorkflowNoStatus_NotFound(t *testing.T) {
 				UID:        types.UID(job.UUID),
 			}},
 		}})
-	err := manager.ReportWorkflowResource(workflow)
+	err := manager.ReportWorkflowResource(context.Background(), workflow)
 	assert.Nil(t, err)
 
 	_, err = manager.ReadArtifact("run-1", "node-1", "artifact-1")
@@ -2029,8 +2164,257 @@ func TestReadArtifact_NoRun_NotFound(t *testing.T) {
 }
 
 const (
+	v2compatPipeline = `
+apiVersion: argoproj.io/v1alpha1
+kind: Workflow
+metadata:
+  generateName: two-step-pipeline-
+  annotations:
+    pipelines.kubeflow.org/kfp_sdk_version: 1.6.4
+    pipelines.kubeflow.org/pipeline_compilation_time: '2021-07-14T06:59:20.208189'
+    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "", "name": "pipeline-root"},
+      {"default": "pipeline/two_step_pipeline", "name": "pipeline-name"}], "name":
+      "two_step_pipeline"}'
+    pipelines.kubeflow.org/v2_pipeline: "true"
+  labels:
+    pipelines.kubeflow.org/v2_pipeline: "true"
+    pipelines.kubeflow.org/kfp_sdk_version: 1.6.4
+spec:
+  entrypoint: two-step-pipeline
+  templates:
+  - name: preprocess
+    container:
+      args:
+      - sh
+      - -ec
+      - |
+        program_path=$(mktemp)
+        printf "%s" "$0" > "$program_path"
+        python3 -u "$program_path" "$@"
+      - |
+        def _make_parent_dirs_and_return_path(file_path: str):
+            import os
+            os.makedirs(os.path.dirname(file_path), exist_ok=True)
+            return file_path
+
+        def preprocess(
+            uri, some_int, output_parameter_one,
+            output_dataset_one
+        ):
+            '''Dummy Preprocess Step.'''
+            with open(output_dataset_one, 'w') as f:
+                f.write('Output dataset')
+            with open(output_parameter_one, 'w') as f:
+                f.write("{}".format(1234))
+
+        import argparse
+        _parser = argparse.ArgumentParser(prog='Preprocess', description='Dummy Preprocess Step.')
+        _parser.add_argument("--uri", dest="uri", type=str, required=True, default=argparse.SUPPRESS)
+        _parser.add_argument("--some-int", dest="some_int", type=int, required=True, default=argparse.SUPPRESS)
+        _parser.add_argument("--output-parameter-one", dest="output_parameter_one", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
+        _parser.add_argument("--output-dataset-one", dest="output_dataset_one", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
+        _parsed_args = vars(_parser.parse_args())
+
+        _outputs = preprocess(**_parsed_args)
+      - --uri
+      - '{{$.inputs.parameters[''uri'']}}'
+      - --some-int
+      - '{{$.inputs.parameters[''some_int'']}}'
+      - --output-parameter-one
+      - '{{$.outputs.parameters[''output_parameter_one''].output_file}}'
+      - --output-dataset-one
+      - '{{$.outputs.artifacts[''output_dataset_one''].path}}'
+      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
+        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
+        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess, --pipeline_name,
+        '{{inputs.parameters.pipeline-name}}', --pipeline_run_id, $(WORKFLOW_ID),
+        --pipeline_task_id, $(KFP_POD_NAME), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
+        --, some_int=12, uri=uri-to-import, --]
+      env:
+      - name: KFP_POD_NAME
+        valueFrom:
+          fieldRef: {fieldPath: metadata.name}
+      - name: KFP_NAMESPACE
+        valueFrom:
+          fieldRef: {fieldPath: metadata.namespace}
+      - name: WORKFLOW_ID
+        valueFrom:
+          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
+      - name: ENABLE_CACHING
+        valueFrom:
+          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
+      - {name: KFP_V2_IMAGE, value: 'python:3.9'}
+      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"some_int": {"type":
+          "INT"}, "uri": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
+          {"output_parameter_one": {"type": "INT", "path": "/tmp/outputs/output_parameter_one/data"}},
+          "outputArtifacts": {"output_dataset_one": {"schemaTitle": "system.Dataset",
+          "instanceSchema": "", "metadataPath": "/tmp/outputs/output_dataset_one/data"}}}'}
+      envFrom:
+      - configMapRef: {name: metadata-grpc-configmap, optional: true}
+      image: python:3.9
+      volumeMounts:
+      - {mountPath: /kfp-launcher, name: kfp-launcher}
+    inputs:
+      parameters:
+      - {name: pipeline-name}
+      - {name: pipeline-root}
+    outputs:
+      parameters:
+      - name: preprocess-output_parameter_one
+        valueFrom: {path: /tmp/outputs/output_parameter_one/data}
+      artifacts:
+      - {name: preprocess-output_dataset_one, path: /tmp/outputs/output_dataset_one/data}
+      - {name: preprocess-output_parameter_one, path: /tmp/outputs/output_parameter_one/data}
+    metadata:
+      annotations:
+        pipelines.kubeflow.org/v2_component: "true"
+        pipelines.kubeflow.org/component_ref: '{}'
+        pipelines.kubeflow.org/arguments.parameters: '{"some_int": "12", "uri": "uri-to-import"}'
+      labels:
+        pipelines.kubeflow.org/kfp_sdk_version: 1.6.4
+        pipelines.kubeflow.org/pipeline-sdk-type: kfp
+        pipelines.kubeflow.org/v2_component: "true"
+        pipelines.kubeflow.org/enable_caching: "true"
+    initContainers:
+    - command: [/bin/mount_launcher.sh]
+      image: gcr.io/ml-pipeline/kfp-launcher:1.6.4
+      name: kfp-launcher
+      mirrorVolumeMounts: true
+    volumes:
+    - {name: kfp-launcher}
+  - name: train-op
+    container:
+      args:
+      - sh
+      - -ec
+      - |
+        program_path=$(mktemp)
+        printf "%s" "$0" > "$program_path"
+        python3 -u "$program_path" "$@"
+      - |
+        def _make_parent_dirs_and_return_path(file_path: str):
+            import os
+            os.makedirs(os.path.dirname(file_path), exist_ok=True)
+            return file_path
+
+        def train_op(
+            dataset,
+            model,
+            num_steps = 100
+        ):
+            '''Dummy Training Step.'''
+
+            with open(dataset, 'r') as input_file:
+                input_string = input_file.read()
+                with open(model, 'w') as output_file:
+                    for i in range(num_steps):
+                        output_file.write(
+                            "Step {}\n{}\n=====\n".format(i, input_string)
+                        )
+
+        import argparse
+        _parser = argparse.ArgumentParser(prog='Train op', description='Dummy Training Step.')
+        _parser.add_argument("--dataset", dest="dataset", type=str, required=True, default=argparse.SUPPRESS)
+        _parser.add_argument("--num-steps", dest="num_steps", type=int, required=False, default=argparse.SUPPRESS)
+        _parser.add_argument("--model", dest="model", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
+        _parsed_args = vars(_parser.parse_args())
+
+        _outputs = train_op(**_parsed_args)
+      - --dataset
+      - '{{$.inputs.artifacts[''dataset''].path}}'
+      - --num-steps
+      - '{{$.inputs.parameters[''num_steps'']}}'
+      - --model
+      - '{{$.outputs.artifacts[''model''].path}}'
+      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
+        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
+        --container_image, $(KFP_V2_IMAGE), --task_name, train-op, --pipeline_name,
+        '{{inputs.parameters.pipeline-name}}', --pipeline_run_id, $(WORKFLOW_ID),
+        --pipeline_task_id, $(KFP_POD_NAME), --pipeline_root, '{{inputs.parameters.pipeline-root}}',
+        --, 'num_steps={{inputs.parameters.preprocess-output_parameter_one}}', --]
+      env:
+      - name: KFP_POD_NAME
+        valueFrom:
+          fieldRef: {fieldPath: metadata.name}
+      - name: KFP_NAMESPACE
+        valueFrom:
+          fieldRef: {fieldPath: metadata.namespace}
+      - name: WORKFLOW_ID
+        valueFrom:
+          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
+      - name: ENABLE_CACHING
+        valueFrom:
+          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
+      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
+      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"num_steps": {"type":
+          "INT"}}, "inputArtifacts": {"dataset": {"metadataPath": "/tmp/inputs/dataset/data",
+          "schemaTitle": "system.Dataset", "instanceSchema": ""}}, "outputParameters":
+          {}, "outputArtifacts": {"model": {"schemaTitle": "system.Model", "instanceSchema":
+          "", "metadataPath": "/tmp/outputs/model/data"}}}'}
+      envFrom:
+      - configMapRef: {name: metadata-grpc-configmap, optional: true}
+      image: python:3.7
+      volumeMounts:
+      - {mountPath: /kfp-launcher, name: kfp-launcher}
+    inputs:
+      parameters:
+      - {name: pipeline-name}
+      - {name: pipeline-root}
+      - {name: preprocess-output_parameter_one}
+      artifacts:
+      - {name: preprocess-output_dataset_one, path: /tmp/inputs/dataset/data}
+    outputs:
+      artifacts:
+      - {name: train-op-model, path: /tmp/outputs/model/data}
+    metadata:
+      annotations:
+        pipelines.kubeflow.org/v2_component: "true"
+        pipelines.kubeflow.org/component_ref: '{}'
+        pipelines.kubeflow.org/arguments.parameters: '{"num_steps": "{{inputs.parameters.preprocess-output_parameter_one}}"}'
+      labels:
+        pipelines.kubeflow.org/kfp_sdk_version: 1.6.4
+        pipelines.kubeflow.org/pipeline-sdk-type: kfp
+        pipelines.kubeflow.org/v2_component: "true"
+        pipelines.kubeflow.org/enable_caching: "true"
+    initContainers:
+    - command: [/bin/mount_launcher.sh]
+      image: gcr.io/ml-pipeline/kfp-launcher:1.6.4
+      name: kfp-launcher
+      mirrorVolumeMounts: true
+    volumes:
+    - {name: kfp-launcher}
+  - name: two-step-pipeline
+    inputs:
+      parameters:
+      - {name: pipeline-name}
+      - {name: pipeline-root}
+    dag:
+      tasks:
+      - name: preprocess
+        template: preprocess
+        arguments:
+          parameters:
+          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
+          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
+      - name: train-op
+        template: train-op
+        dependencies: [preprocess]
+        arguments:
+          parameters:
+          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
+          - {name: pipeline-root, value: '{{inputs.parameters.pipeline-root}}'}
+          - {name: preprocess-output_parameter_one, value: '{{tasks.preprocess.outputs.parameters.preprocess-output_parameter_one}}'}
+          artifacts:
+          - {name: preprocess-output_dataset_one, from: '{{tasks.preprocess.outputs.artifacts.preprocess-output_dataset_one}}'}
+  arguments:
+    parameters:
+    - {name: pipeline-root, value: ''}
+    - {name: pipeline-name, value: pipeline/two_step_pipeline}
+  serviceAccountName: pipeline-runner
+`
+
 	complexPipeline = `
-# Copyright 2018 Google LLC
+# Copyright 2018 The Kubeflow Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -2398,7 +2782,8 @@ func TestCreatePipelineVersion(t *testing.T) {
 	pipelineStore.SetUUIDGenerator(util.NewFakeUUIDGeneratorOrFatal(FakeUUIDOne, nil))
 	version, err := manager.CreatePipelineVersion(
 		&api.PipelineVersion{
-			Name: "p_v",
+			Name:        "p_v",
+			Description: "test",
 			ResourceReferences: []*api.ResourceReference{
 				&api.ResourceReference{
 					Key: &api.ResourceKey{
@@ -2420,6 +2805,7 @@ func TestCreatePipelineVersion(t *testing.T) {
 		Parameters:     "[{\"name\":\"param1\"}]",
 		Status:         model.PipelineVersionReady,
 		PipelineId:     DefaultFakeUUID,
+		Description:    "test",
 	}
 	assert.Equal(t, pipelineVersionExpected, version)
 }
@@ -2460,6 +2846,56 @@ func TestCreatePipelineVersion_ComplexPipelineVersion(t *testing.T) {
 	assert.Nil(t, err)
 }
 
+func TestCreatePipelineVersion_V2PipelineName(t *testing.T) {
+	tests := []struct {
+		name         string
+		namespace    string
+		pipelineName string
+	}{
+		{name: "v2-compat", namespace: "", pipelineName: "pipeline/v2-compat"},
+		{name: "pipe3", namespace: "", pipelineName: "pipeline/pipe3"},
+		{name: "pipeline2", namespace: "kubeflow", pipelineName: "namespace/kubeflow/pipeline/pipeline2"},
+		{name: "abcd", namespace: "user", pipelineName: "namespace/user/pipeline/abcd"},
+	}
+	for _, test := range tests {
+		t.Run(fmt.Sprintf("%+v", test), func(t *testing.T) {
+			store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
+			defer store.Close()
+			manager := NewResourceManager(store)
+
+			createdPipeline, err := manager.CreatePipeline(test.name, "", test.namespace, []byte(strings.TrimSpace(
+				v2compatPipeline)))
+			require.Nil(t, err)
+			pipelineStore, ok := store.pipelineStore.(*storage.PipelineStore)
+			require.True(t, ok)
+			pipelineStore.SetUUIDGenerator(util.NewFakeUUIDGeneratorOrFatal(FakeUUIDOne, nil))
+			version, err := manager.CreatePipelineVersion(
+				&api.PipelineVersion{
+					Name: "pipeline_version",
+					ResourceReferences: []*api.ResourceReference{{
+						Key: &api.ResourceKey{
+							Id:   createdPipeline.UUID,
+							Type: api.ResourceType_PIPELINE,
+						},
+						Relationship: api.Relationship_OWNER,
+					}},
+				},
+				[]byte(strings.TrimSpace(v2compatPipeline)), true)
+			require.Nil(t, err)
+			params, err := util.UnmarshalParameters(version.Parameters)
+			require.Nil(t, err)
+			var nameParam *v1alpha1.Parameter
+			for _, param := range params {
+				if param.Name == "pipeline-name" {
+					nameParam = &param
+				}
+			}
+			require.NotNil(t, nameParam)
+			require.Equal(t, test.pipelineName, nameParam.Value.String())
+		})
+	}
+}
+
 func TestCreatePipelineVersion_CreatePipelineVersionFileError(t *testing.T) {
 	store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
 	defer store.Close()
@@ -2499,7 +2935,7 @@ func TestCreatePipelineVersion_CreatePipelineVersionFileError(t *testing.T) {
 	assert.NotNil(t, version)
 }
 
-func TestCreatePipelineVersion_GetParametersError(t *testing.T) {
+func TestCreatePipelineVersion_ParseWorkflowError(t *testing.T) {
 	store := NewFakeClientManagerOrFatal(util.NewFakeTimeForEpoch())
 	defer store.Close()
 	manager := NewResourceManager(store)
@@ -2527,7 +2963,7 @@ func TestCreatePipelineVersion_GetParametersError(t *testing.T) {
 		},
 		[]byte("I am invalid yaml"), true)
 	assert.Equal(t, codes.InvalidArgument, err.(*util.UserError).ExternalStatusCode())
-	assert.Contains(t, err.Error(), "Failed to parse the parameter")
+	assert.Contains(t, err.Error(), "Failed to parse the workflow")
 }
 
 func TestCreatePipelineVersion_StorePipelineVersionMetadataError(t *testing.T) {
@@ -2692,3 +3128,35 @@ func TestCreateDefaultExperiment_MultiUser(t *testing.T) {
 	}
 	assert.Equal(t, expectedExperiment, experiment)
 }
+
+func TestCreateTask(t *testing.T) {
+	_, manager, _, _, runDetail := initWithExperimentAndPipelineAndRun(t)
+	task := &api.Task{
+		Namespace:       "",
+		PipelineName:    "pipeline/my-pipeline",
+		RunId:           runDetail.UUID,
+		MlmdExecutionID: "1",
+		CreatedAt:       &timestamp.Timestamp{Seconds: 1462875553},
+		FinishedAt:      &timestamp.Timestamp{Seconds: 1462875663},
+		Fingerprint:     "123",
+	}
+
+	expectedTask := &model.Task{
+		UUID:              DefaultFakeUUID,
+		Namespace:         "",
+		PipelineName:      "pipeline/my-pipeline",
+		RunUUID:           runDetail.UUID,
+		MLMDExecutionID:   "1",
+		CreatedTimestamp:  1462875553,
+		FinishedTimestamp: 1462875663,
+		Fingerprint:       "123",
+	}
+	createdTask, err := manager.CreateTask(context.Background(), task)
+	assert.Nil(t, err)
+	assert.Equal(t, expectedTask, createdTask, "The CreateTask return has unexpected value.")
+
+	// Verify the T in DB is in status PipelineVersionCreating.
+	storedTask, err := manager.taskStore.GetTask(DefaultFakeUUID)
+	assert.Nil(t, err)
+	assert.Equal(t, expectedTask, storedTask, "The StoredTask return has unexpected value.")
+}
