{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST End to End examples with Kubeflow compoenents\n",
    "\n",
    "This pipeline contains 5 steps, it finds the best hyperparameter using Katib, creates PVC for storing models, processes the hyperparameter results, distributedly trains the model on TFJob with the best hyperparameter using more iterations, and finally serves the model using KFServing. You can visit this [medium blog](https://medium.com/@liuhgxa/an-end-to-end-use-case-by-kubeflow-b2f72b0b587) for more details on this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model name, Kubeflow user namespace, and storageclass for running this pipeline\n",
    "\n",
    "Change the below cell with a name you want to use for this pipeline, which namespace you want to execute on Kubeflow, and the [storageclass](https://kubernetes.io/docs/concepts/storage/storage-classes/) to run and store the trained model. **Make sure the storageclass below supports ReadWriteMany in order to run distributed training.** \n",
    "\n",
    "The default storageclass value below will work on IBM Cloud. For other cloud providers such as AWS and GCP, find out the default storageclass by running the bash command `kubectl get storageclass`, and replace the storageclass variable default value below. If the Kubernetes Cluster doesn't have any storageclass, simply assign the storageclass variable below as `''` to use the generic Kubernetes persistent storage claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibmc-file-gold is the recommended ReadWriteMany storageclass for IBM Cloud.\n",
    "storageclass = 'ibmc-file-gold'\n",
    "model_name = \"mnist-demo\"\n",
    "user_namespace = \"anonymous\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the DSL package and define the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from string import Template\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp.components import func_to_container_op\n",
    "import kfp.dsl as dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mnist_experiment_result(experiment_result) -> str:\n",
    "    import json\n",
    "    r = json.loads(experiment_result)\n",
    "    args = []\n",
    "    for hp in r:\n",
    "        print(hp)\n",
    "        args.append(\"%s=%s\" % (hp[\"name\"], hp[\"value\"]))\n",
    "\n",
    "    return \" \".join(args)\n",
    "\n",
    "def add_istio_annotation(op):\n",
    "    op.add_pod_annotation(name='sidecar.istio.io/inject', value='false')\n",
    "    return op\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End to end pipeline\",\n",
    "    description=\"An end to end example including hyperparameter tuning, train and inference.\"\n",
    ")\n",
    "def mnist_pipeline(\n",
    "        name=model_name,\n",
    "        namespace=user_namespace,\n",
    "        storageclass=storageclass,\n",
    "        step=4000):\n",
    "    # step 1: create a Katib experiment to tune hyperparameters\n",
    "    objectiveConfig = {\n",
    "      \"type\": \"minimize\",\n",
    "      \"goal\": 0.001,\n",
    "      \"objectiveMetricName\": \"loss\",\n",
    "    }\n",
    "    algorithmConfig = {\"algorithmName\" : \"random\"}\n",
    "    parameters = [\n",
    "      {\"name\": \"--tf-learning-rate\", \"parameterType\": \"double\", \"feasibleSpace\": {\"min\": \"0.01\",\"max\": \"0.03\"}},\n",
    "      {\"name\": \"--tf-batch-size\", \"parameterType\": \"discrete\", \"feasibleSpace\": {\"list\": [\"16\", \"32\", \"64\"]}},\n",
    "    ]\n",
    "    rawTemplate = {\n",
    "      \"apiVersion\": \"kubeflow.org/v1\",\n",
    "      \"kind\": \"TFJob\",\n",
    "      \"metadata\": {\n",
    "         \"name\": \"{{.Trial}}\",\n",
    "         \"namespace\": \"{{.NameSpace}}\"\n",
    "      },\n",
    "      \"spec\": {\n",
    "        \"tfReplicaSpecs\": {\n",
    "          \"Chief\": {\n",
    "            \"replicas\": 1,\n",
    "            \"restartPolicy\": \"OnFailure\",\n",
    "            \"template\": {\n",
    "              \"spec\": {\n",
    "                \"containers\": [\n",
    "                {\n",
    "                  \"command\": [\n",
    "                    \"sh\",\n",
    "                    \"-c\"\n",
    "                  ],\n",
    "                  \"args\": [\n",
    "                    \"python /opt/model.py --tf-train-steps=2000 {{- with .HyperParameters}} {{- range .}} {{.Name}}={{.Value}} {{- end}} {{- end}}\"\n",
    "                  ],\n",
    "                  \"image\": \"liuhougangxa/tf-estimator-mnist\",\n",
    "                  \"name\": \"tensorflow\"\n",
    "                }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"Worker\": {\n",
    "            \"replicas\": 3,\n",
    "            \"restartPolicy\": \"OnFailure\",\n",
    "            \"template\": {\n",
    "              \"spec\": {\n",
    "                \"containers\": [\n",
    "                {\n",
    "                  \"command\": [\n",
    "                    \"sh\",\n",
    "                    \"-c\"\n",
    "                  ],\n",
    "                  \"args\": [ \n",
    "                    \"python /opt/model.py --tf-train-steps=2000 {{- with .HyperParameters}} {{- range .}} {{.Name}}={{.Value}} {{- end}} {{- end}}\"\n",
    "                  ],\n",
    "                  \"image\": \"liuhougangxa/tf-estimator-mnist\",\n",
    "                  \"name\": \"tensorflow\"\n",
    "                }\n",
    "                ]\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    trialTemplate = {\n",
    "      \"goTemplate\": {\n",
    "        \"rawTemplate\": json.dumps(rawTemplate)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    metricsCollectorSpec = {\n",
    "      \"source\": {\n",
    "        \"fileSystemPath\": {\n",
    "          \"path\": \"/tmp/tf\",\n",
    "          \"kind\": \"Directory\"\n",
    "        }\n",
    "      },\n",
    "      \"collector\": {\n",
    "        \"kind\": \"TensorFlowEvent\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "    katib_experiment_launcher_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/master/components/kubeflow/katib-launcher/component.yaml')\n",
    "    op1 = katib_experiment_launcher_op(\n",
    "            experiment_name=name,\n",
    "            experiment_namespace=namespace,\n",
    "            parallel_trial_count=3,\n",
    "            max_trial_count=12,\n",
    "            objective=str(objectiveConfig),\n",
    "            algorithm=str(algorithmConfig),\n",
    "            trial_template=str(trialTemplate),\n",
    "            parameters=str(parameters),\n",
    "            metrics_collector=str(metricsCollectorSpec),\n",
    "            # experiment_timeout_minutes=experimentTimeoutMinutes,\n",
    "            delete_finished_experiment=False)\n",
    "\n",
    "    # step2: create a TFJob to train your model with best hyperparameter tuned by Katib\n",
    "    tfjobjson_template = Template(\"\"\"\n",
    "{\n",
    "  \"apiVersion\": \"kubeflow.org/v1\",\n",
    "  \"kind\": \"TFJob\",\n",
    "  \"metadata\": {\n",
    "    \"name\": \"$name\",\n",
    "    \"namespace\": \"$namespace\",\n",
    "    \"annotations\": {\n",
    "        \"sidecar.istio.io/inject\": \"false\"\n",
    "    }\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"tfReplicaSpecs\": {\n",
    "      \"Chief\": {\n",
    "        \"replicas\": 1,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "          \"metadata\": {\n",
    "            \"annotations\": {\n",
    "              \"sidecar.istio.io/inject\": \"false\"\n",
    "            }\n",
    "          },\n",
    "          \"spec\": {\n",
    "            \"volumes\": [\n",
    "              {\n",
    "                \"name\": \"export-model\",\n",
    "                \"persistentVolumeClaim\": {\n",
    "                  \"claimName\": \"$modelpvc\"\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            \"containers\": [\n",
    "              {\n",
    "                \"command\": [\n",
    "                  \"sh\",\n",
    "                  \"-c\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                  \"python /opt/model.py --tf-train-steps=$step --tf-export-dir=/mnt/export $args\"\n",
    "                ],\n",
    "                \"image\": \"liuhougangxa/tf-estimator-mnist\",\n",
    "                \"name\": \"tensorflow\",\n",
    "                \"volumeMounts\": [\n",
    "                  {\n",
    "                    \"mountPath\": \"/mnt/export\",\n",
    "                    \"name\": \"export-model\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"Worker\": {\n",
    "        \"replicas\": 3,\n",
    "        \"restartPolicy\": \"OnFailure\",\n",
    "        \"template\": {\n",
    "          \"metadata\": {\n",
    "            \"annotations\": {\n",
    "              \"sidecar.istio.io/inject\": \"false\"\n",
    "            }\n",
    "          },\n",
    "          \"spec\": {\n",
    "            \"volumes\": [\n",
    "              {\n",
    "                \"name\": \"export-model\",\n",
    "                \"persistentVolumeClaim\": {\n",
    "                  \"claimName\": \"$modelpvc\"\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            \"containers\": [\n",
    "              {\n",
    "                \"command\": [\n",
    "                  \"sh\",\n",
    "                  \"-c\"\n",
    "                ],\n",
    "                \"args\": [\n",
    "                  \"python /opt/model.py --tf-train-steps=$step --tf-export-dir=/mnt/export $args\"\n",
    "                ],\n",
    "                \"image\": \"liuhougangxa/tf-estimator-mnist\",\n",
    "                \"name\": \"tensorflow\",\n",
    "                \"volumeMounts\": [\n",
    "                  {\n",
    "                    \"mountPath\": \"/mnt/export\",\n",
    "                    \"name\": \"export-model\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "    convert_op = func_to_container_op(convert_mnist_experiment_result)\n",
    "    op2 = convert_op(op1.output)\n",
    "    \n",
    "    volume_template = Template(\"\"\"\n",
    "{\n",
    "  \"apiVersion\": \"v1\",\n",
    "  \"kind\": \"PersistentVolumeClaim\",\n",
    "  \"metadata\": {\n",
    "    \"name\": \"{{workflow.name}}-modelpvc\",\n",
    "    \"namespace\": \"$namespace\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "      \"accessModes\": [\"ReadWriteMany\"],\n",
    "      \"resources\": {\n",
    "          \"requests\": {\n",
    "              \"storage\": \"1Gi\"\n",
    "          }\n",
    "      },\n",
    "      \"storageClassName\": \"$storageclass\"\n",
    "   }\n",
    "}\n",
    "\"\"\")\n",
    "    \n",
    "    volopjson = volume_template.substitute({'namespace': namespace, 'storageclass': storageclass})\n",
    "    volop = json.loads(volopjson)\n",
    "\n",
    "    modelvolop = dsl.ResourceOp(\n",
    "        name=\"modelpvc\",\n",
    "        k8s_resource=volop\n",
    "    )\n",
    "\n",
    "    tfjobjson = tfjobjson_template.substitute(\n",
    "            {'args': op2.output,\n",
    "             'name': name,\n",
    "             'namespace': namespace,\n",
    "             'step': step,\n",
    "             'modelpvc': modelvolop.outputs[\"name\"]\n",
    "            })\n",
    "\n",
    "    tfjob = json.loads(tfjobjson)\n",
    "\n",
    "    train = dsl.ResourceOp(\n",
    "        name=\"train\",\n",
    "        k8s_resource=tfjob,\n",
    "        success_condition='status.replicaStatuses.Worker.succeeded==3,status.replicaStatuses.Chief.succeeded==1'\n",
    "    )\n",
    "\n",
    "    # step 3: model inferencese by KFServing Inferenceservice\n",
    "    inferenceservice_template = Template(\"\"\"\n",
    "{\n",
    "  \"apiVersion\": \"serving.kubeflow.org/v1alpha2\",\n",
    "  \"kind\": \"InferenceService\",\n",
    "  \"metadata\": {\n",
    "    \"name\": \"$name\",\n",
    "    \"namespace\": \"$namespace\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"default\": {\n",
    "      \"predictor\": {\n",
    "        \"tensorflow\": {\n",
    "          \"storageUri\": \"pvc://$modelpvc/\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "    inferenceservicejson = inferenceservice_template.substitute({'modelpvc': modelvolop.outputs[\"name\"],\n",
    "                                                                 'name': name,\n",
    "                                                                 'namespace': namespace})\n",
    "    inferenceservice =  json.loads(inferenceservicejson)\n",
    "    inference = dsl.ResourceOp(\n",
    "      name=\"inference\",\n",
    "      k8s_resource=inferenceservice,\n",
    "      success_condition='status.url').after(train)\n",
    "    \n",
    "    dsl.get_pipeline_conf().add_op_transformer(add_istio_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign permission to Kubeflow pipeline service account and run this pipeline using the kfp-tekton SDK. You can skip the below command if you are runnning with multi-user mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create clusterrolebinding $user_namespace-admin --clusterrole cluster-admin --serviceaccount=kubeflow:pipeline-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Kubeflow Pipeline Host\n",
    "host=None\n",
    "\n",
    "# Submit a pipeline run\n",
    "from kfp_tekton import TektonClient\n",
    "TektonClient(host=host).create_run_from_pipeline_func(mnist_pipeline, arguments={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the pipeline done, you can get `inferenceservice` name using the below command, for example in this case in my cluster, the `inference-name` is `mnist-demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get inferenceservice -n $user_namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a mnist picture for inference test if it's not in this directory, such as 9.bmp from [here](https://raw.githubusercontent.com/hougangliu/pipelines/e2e-pipeline-sample/samples/contrib/e2e-mnist/9.bmp). Then upload it to the notebook.\n",
    "\n",
    "Update the **istio_ingress_gateway** below with your kfserving ingress endpoint. Then, execute the below cell to send a sample payload to the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Get istio_ingress_gateway endpoint by \"kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\"\n",
    "istio_ingress_gateway = \"xxx.xx.xx.xxx\"\n",
    "\n",
    "# Get inference_name as above step 1\n",
    "inference_name = model_name\n",
    "inference_namespace = user_namespace\n",
    "# image_file is the mnist picture uploaded as above step 2\n",
    "image_file = '9.bmp'\n",
    "data = np.array(Image.open(image_file).convert('L').resize((28, 28))).astype(np.float).reshape(-1, 28, 28, 1)\n",
    "np.set_printoptions(threshold=np.inf)       \n",
    "json_request = '{{ \"instances\" : {} }}'.format(np.array2string(data, separator=',', formatter={'float':lambda x: \"%.1f\" % x}))\n",
    "headers={\"Host\": \"%s.%s.example.com\" % (inference_name, inference_namespace)}\n",
    "\n",
    "response = requests.post(\"http://%s/v1/models/%s:predict\" % (istio_ingress_gateway, inference_name), data = json_request, headers = headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "Due to Tekton lacking the exit operation support. We need to run the below commands to clean up the resources from this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete inferenceservice -n $user_namespace $model_name\n",
    "!kubectl delete experiment -n $user_namespace $model_name\n",
    "!kubectl delete tfjob -n $user_namespace $model_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
