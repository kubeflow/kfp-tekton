apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-config
  namespace: kube-system
data:
  fluent.conf: |-
    <source>
      @type tail
      path "/var/log/containers/*.log"
      pos_file /var/log/fluentd-docker.pos
      time_format %Y-%m-%dT%H:%M:%S
      tag kubernetes.*
      format json
      read_from_head true
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>

    <match kubernetes.**>
      @type rewrite_tag_filter
      <rule>
        key $['kubernetes']['labels']['app_kubernetes_io/managed-by']
        pattern /^.*tekton-pipelines.*$/
        tag tekton-pipelines
      </rule>
    </match>

    <filter tekton-pipelines>
      @type record_transformer
      enable_ruby
      <record>
        pipelinerun ${record["kubernetes"]["labels"]["tekton_dev/pipelineRun"]}
        pipelinetask ${record["kubernetes"]["labels"]["tekton_dev/pipelineTask"]}
      </record>
    </filter>

    # <match tekton-pipelines>
    #   @type stdout
    # </match>

    <match tekton-pipelines>
      @type s3
      aws_key_id "#{ENV['AWS_KEY_ID']}"             # variabel of aws_key_id, e.g: admin123
      aws_sec_key "#{ENV['AWS_SEC_KEY']}"           # variabel of aws_sec_key, e.g: admin123
      s3_endpoint "#{ENV['S3_ENDPOINT']}"           # variabel of s3_endpoint, if you are using minio, e.g: http://9.21.53.162:31846/
      force_path_style "#{ENV['FORCE_PATH_STYLE']}" # This prevents AWS SDK from breaking endpoint URL, set true if you are using minio
      s3_bucket "#{ENV['S3_BUCKET']}"               # variabel of s3_bucket, e.g: mlpipeline
      s3_region "#{ENV['S3_REGION']}"               # variable of s3_region, e.g: us-east-1 or test_region
      path artifact/${$.pipelinerun}/${$.pipelinetask}/${$.kubernetes.pod_name}
      # s3_object_key_format %{path}%{time_slice}_%{index}.log
      time_slice_format %Y%m%d%H%M                  # This timestamp is added to each file name
      <buffer time, $.pipelinerun, $.pipelinetask, $.kubernetes.pod_name>
        @type file
        path /var/log/td-agent/s3
        timekey 120s                                # Flush the accumulated chunks every xx
        timekey_wait 60s                            # Wait for xx seconds before flushing
        timekey_use_utc true                        # Use this option if you prefer UTC timestamps
        chunk_limit_size 30m                        # The maximum size of each chunk
      </buffer>
    </match>
