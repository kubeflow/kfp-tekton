apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  annotations:
    pipelines.kubeflow.org/pipeline_spec: '{"name": "Text splitting pipeline"}'
    sidecar.istio.io/inject: 'false'
    tekton.dev/input_artifacts: '{}'
    tekton.dev/output_artifacts: '{}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: text-splitting-pipeline
spec:
  pipelineSpec:
    tasks:
    - name: split-text-lines
      taskSpec:
        stepTemplate:
          volumeMounts:
          - mountPath: /tmp/inputs/source
            name: source
        steps:
        - image: busybox
          name: copy-inputs
          script: '#!/bin/sh

            set -exo pipefail

            echo -n "one

            two

            three

            four

            five

            six

            seven

            eight

            nine

            ten" > /tmp/inputs/source/data

            '
        - args:
          - --source
          - /tmp/inputs/source/data
          - --odd-lines
          - $(workspaces.split-text-lines.path)/split-text-lines-odd_lines
          - --even-lines
          - $(workspaces.split-text-lines.path)/split-text-lines-even_lines
          command:
          - python3
          - -u
          - -c
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef split_text_lines(source_path , odd_lines_path , even_lines_path\
            \ ):\n    with open(source_path, 'r') as reader:\n        with open(odd_lines_path,\
            \ 'w') as odd_writer:\n            with open(even_lines_path, 'w') as\
            \ even_writer:\n                while True:\n                    line\
            \ = reader.readline()\n                    if line == \"\":\n        \
            \                break\n                    odd_writer.write(line)\n \
            \                   line = reader.readline()\n                    if line\
            \ == \"\":\n                        break\n                    even_writer.write(line)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Split text\
            \ lines', description='')\n_parser.add_argument(\"--source\", dest=\"\
            source_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --odd-lines\", dest=\"odd_lines_path\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--even-lines\"\
            , dest=\"even_lines_path\", type=_make_parent_dirs_and_return_path, required=True,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ split_text_lines(**_parsed_args)\n\n_output_serializers = [\n\n]\n\n\
            import os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
            \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
            \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        volumes:
        - emptyDir: {}
          name: source
        workspaces:
        - name: split-text-lines
      workspaces:
      - name: split-text-lines
        workspace: text-splitting-pipeline
    - name: print-text
      runAfter:
      - split-text-lines
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text.path)/split-text-lines-odd_lines
          command:
          - python3
          - -u
          - -c
          - "def print_text(text_path ): # The \"text\" input is untyped so that any\
            \ data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end = '')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text
      workspaces:
      - name: print-text
        workspace: text-splitting-pipeline
    - name: print-text-2
      runAfter:
      - split-text-lines
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text-2.path)/split-text-lines-even_lines
          command:
          - python3
          - -u
          - -c
          - "def print_text(text_path ): # The \"text\" input is untyped so that any\
            \ data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end = '')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text-2
      workspaces:
      - name: print-text-2
        workspace: text-splitting-pipeline
    workspaces:
    - name: text-splitting-pipeline
  workspaces:
  - name: text-splitting-pipeline
    persistentVolumeClaim:
      claimName: text-splitting-pipeline
