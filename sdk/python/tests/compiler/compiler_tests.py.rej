diff a/sdk/python/tests/compiler/compiler_tests.py b/sdk/python/tests/compiler/compiler_tests.py	(rejected hunks)
@@ -1,4 +1,4 @@
-# Copyright 2018-2019 Google LLC
+# Copyright 2018-2019 The Kubeflow Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -11,7 +11,8 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import List
+from typing import Optional
+import unittest
 
 import kfp
 import kfp.compiler as compiler
@@ -24,9 +25,10 @@ import sys
 import zipfile
 import tarfile
 import tempfile
-import unittest
+import mock
 import yaml
 
+from absl.testing import parameterized
 from kfp.compiler import Compiler
 from kfp.dsl._component import component
 from kfp.dsl import ContainerOp, pipeline, PipelineParam
@@ -43,7 +45,7 @@ def some_op():
   )
 
 
-class TestCompiler(unittest.TestCase):
+class TestCompiler(parameterized.TestCase):
   # Define the places of samples covered by unit tests.
   core_sample_path = os.path.join(os.path.dirname(__file__), '..', '..', '..',
                                   '..', 'samples', 'core',)
@@ -112,6 +114,9 @@ class TestCompiler(unittest.TestCase):
             {'name': 'echo-merged',
             'valueFrom': {'path': '/tmp/message.txt'}
             }],
+        },
+        'metadata': {
+            'labels': {'pipelines.kubeflow.org/enable_caching': 'true'}
         }
       }
       res_output = {
@@ -150,6 +155,9 @@ class TestCompiler(unittest.TestCase):
             "  name: resource\n"
           ),
           'setOwnerReference': True
+        },
+        'metadata': {
+            'labels': {'pipelines.kubeflow.org/enable_caching': 'true'}
         }
       }
 
@@ -213,8 +221,16 @@ class TestCompiler(unittest.TestCase):
       with open(os.path.join(test_data_dir, 'basic_no_decorator.yaml'), 'r') as f:
         golden = yaml.safe_load(f)
 
+      name_to_template = {template['name']: template for template in compiled_workflow['spec']['templates']}
+      for k, v in name_to_template.items():
+        if k in ['exiting', 'get-frequent', 'save']:
+          self.assertEqual(v['metadata']['labels']['pipelines.kubeflow.org/pipeline-sdk-type'], 'kfp')
+          self.assertTrue(v['metadata']['labels']['pipelines.kubeflow.org/kfp_sdk_version'] is not None)
+
       for workflow in golden, compiled_workflow:
         del workflow['metadata']
+        for template in workflow['spec']['templates']:
+          template.pop('metadata', None)
 
       self.assertEqual(golden, compiled_workflow)
     finally:
@@ -296,14 +312,16 @@ class TestCompiler(unittest.TestCase):
     finally:
       shutil.rmtree(tmpdir)
 
-  def _test_py_compile_yaml(self, file_base_name):
+  def _test_py_compile_yaml(self, file_base_name: str, mode: Optional[str] = None):
     test_data_dir = os.path.join(os.path.dirname(__file__), 'testdata')
     py_file = os.path.join(test_data_dir, file_base_name + '.py')
     tmpdir = tempfile.mkdtemp()
     try:
       target_yaml = os.path.join(tmpdir, file_base_name + '-pipeline.yaml')
-      subprocess.check_call([
-          'dsl-compile', '--py', py_file, '--output', target_yaml])
+      cmds = ['dsl-compile', '--py', py_file, '--output', target_yaml]
+      if mode:
+        cmds.extend(['--mode', mode])
+      subprocess.check_call(cmds)
       with open(os.path.join(test_data_dir, file_base_name + '.yaml'), 'r') as f:
         golden = yaml.safe_load(f)
 
@@ -315,6 +333,12 @@ class TestCompiler(unittest.TestCase):
         for template in workflow['spec']['templates']:
           template.pop('metadata', None)
 
+          # v2-compat mode uses launcher image with pinned version. Ignore it.
+          if 'initContainers' in template and (
+              template['initContainers'][0]['image'].startswith(
+                  'gcr.io/ml-pipeline/kfp-launcher')):
+            template['initContainers'][0].pop('image', None)
+
       self.maxDiff = None
       self.assertEqual(golden, compiled)
     finally:
@@ -375,6 +399,59 @@ class TestCompiler(unittest.TestCase):
     """Test a pipeline with a volume and volume mount."""
     self._test_py_compile_yaml('volume')
 
+  @parameterized.parameters(
+    {'mode': 'V2_COMPATIBLE', 'is_v2': True},
+    {'mode': 'V1', 'is_v2': False},
+    {'mode': 'V1_LEGACY', 'is_v2': False},
+    {'mode': None, 'is_v2': False},
+    {'mode': 'V2_COMPATIBLE', 'env': 'V1', 'is_v2': True},
+    {'mode': None, 'env': 'V1', 'is_v2': False},
+    {'mode': None, 'env': 'V2_COMPATIBLE', 'is_v2': True},
+    {'mode': None, 'env': 'V1_LEGACY', 'is_v2': False},
+    {'mode': 'INVALID', 'error': True},
+    {'mode': None, 'env': 'INVALID', 'error': True},
+  )
+  def test_dsl_compile_mode(self, mode: Optional[str] = None, is_v2: Optional[bool] = None, env: Optional[str] = None, error: Optional[bool] = None):
+    with mock.patch.dict(os.environ, env and {'KF_PIPELINES_COMPILER_MODE': env} or {}):
+      file_base_name = 'two_step'
+      test_data_dir = os.path.join(os.path.dirname(__file__), 'testdata')
+      py_file = os.path.join(test_data_dir, f'{file_base_name}.py')
+      tmpdir = tempfile.mkdtemp()
+      try:
+        target_yaml = os.path.join(tmpdir, f'{file_base_name}.yaml')
+        args = ['dsl-compile', '--py', py_file, '--output', target_yaml]
+        if mode:
+          args = args + ['--mode', mode]
+        got_error = None
+        compiled = None
+        try:
+          subprocess.check_output(args)
+          with open(target_yaml, 'r') as f:
+            compiled = yaml.safe_load(f)
+        except subprocess.CalledProcessError as err:
+          got_error = err
+        if error:
+          if not got_error:
+            self.fail(f'expected error, but succeeded')
+        else:
+          if got_error:
+            self.fail(f'expected success, but got {got_error}')
+          v2_pipeline_annotation = compiled['metadata']['annotations'].get('pipelines.kubeflow.org/v2_pipeline')
+          if is_v2:
+            self.assertEqual(
+              'true',
+              v2_pipeline_annotation,
+              f'expected to compile in v2_compatible mode'
+            )
+          else:
+            self.assertEqual(
+              None,
+              v2_pipeline_annotation,
+              f'expected to compile in v1 mode'
+            )
+      finally:
+        shutil.rmtree(tmpdir)
+
   def test_py_retry_policy(self):
       """Test retry policy is set."""
 
@@ -398,6 +475,46 @@ class TestCompiler(unittest.TestCase):
       self.assertEqual(template['retryStrategy']['backoff']['maxDuration'], backoff_max_duration)
 
 
+  def test_py_runtime_memory_request(self):
+      """Test memory request."""
+
+      def my_pipeline(memory: str, cpu: str):
+        some_op().set_cpu_request(memory)
+
+      workflow = kfp.compiler.Compiler()._create_workflow(my_pipeline)
+      name_to_template = {template['name']: template for template in workflow['spec']['templates']}
+      main_dag_tasks = name_to_template[workflow['spec']['entrypoint']]['dag']['tasks']
+      template = name_to_template[main_dag_tasks[0]['template']]
+
+      self.assertEqual(template['podSpecPatch'], '{"containers": [{"name": "main", "resources": {"requests": {"cpu": "{{inputs.parameters.memory}}"}}}]}')
+      
+  def test_py_runtime_gpu_request(self):
+      """Test GPU request."""
+
+      def my_pipeline(nbr_gpus: int, gpu_vendor: str):
+        some_op().set_gpu_limit(nbr_gpus, gpu_vendor)
+      
+      workflow = kfp.compiler.Compiler()._create_workflow(my_pipeline)
+      name_to_template = {template['name']: template for template in workflow['spec']['templates']}
+      main_dag_tasks = name_to_template[workflow['spec']['entrypoint']]['dag']['tasks']
+      template = name_to_template[main_dag_tasks[0]['template']]
+
+      self.assertEqual(template['podSpecPatch'], '{"containers": [{"name": "main", "resources": {"limits": {"{{inputs.parameters.gpu_vendor}}": "{{inputs.parameters.nbr_gpus}}"}}}]}')
+      
+  def test_py_runtime_node_selection(self):
+      """Test node selection request."""
+
+      def my_pipeline(constrain_type: str, constrain_value: str):
+        some_op().add_node_selector_constraint(constrain_type, constrain_value)
+      
+      workflow = kfp.compiler.Compiler()._create_workflow(my_pipeline)
+      name_to_template = {template['name']: template for template in workflow['spec']['templates']}
+      main_dag_tasks = name_to_template[workflow['spec']['entrypoint']]['dag']['tasks']
+      template = name_to_template[main_dag_tasks[0]['template']]
+
+      self.assertEqual(template['podSpecPatch'], '{"nodeSelector": [{"{{inputs.parameters.constrain_type}}": "{{inputs.parameters.constrain_value}}"}]}')
+     
+
   def test_py_retry_policy_invalid(self):
       def my_pipeline():
           some_op().set_retry(2, 'Invalid')
@@ -623,6 +740,8 @@ class TestCompiler(unittest.TestCase):
     del compiled_template['name'], expected['name']
     for output in compiled_template['outputs'].get('parameters', []) + compiled_template['outputs'].get('artifacts', []) + expected['outputs'].get('parameters', []) + expected['outputs'].get('artifacts', []):
       del output['name']
+
+    del compiled_template['metadata']
     assert compiled_template == expected
 
   def test_tolerations(self):
@@ -930,7 +1049,7 @@ implementation:
       delete_op_template = [template for template in workflow_dict['spec']['templates'] if template['name'] == 'delete-config-map'][0]
 
       # delete resource operation should not have success condition, failure condition or output parameters.
-      # See https://github.com/argoproj/argo/blob/5331fc02e257266a4a5887dfe6277e5a0b42e7fc/cmd/argoexec/commands/resource.go#L30
+      # See https://github.com/argoproj/argo-workflows/blob/5331fc02e257266a4a5887dfe6277e5a0b42e7fc/cmd/argoexec/commands/resource.go#L30
       self.assertIsNone(delete_op_template.get("successCondition"))
       self.assertIsNone(delete_op_template.get("failureCondition"))
       self.assertDictEqual(delete_op_template.get("outputs", {}), {})
@@ -1118,7 +1237,7 @@ implementation:
     self.assertEqual(resolved, "{{inputs.parameters.op1-param1}}")
 
   def test_uri_artifact_passing(self):
-    self._test_py_compile_yaml('uri_artifacts')
+    self._test_py_compile_yaml('uri_artifacts', mode='V2_COMPATIBLE')
 
   def test_keyword_only_argument_for_pipeline_func(self):
     def some_pipeline(casual_argument: str, *, keyword_only_argument: str):
@@ -1155,3 +1274,20 @@ implementation:
 
     # compare
     self.assertEqual(pipeline_yaml_arg, pipeline_yaml_kwarg)
+
+  def test_use_importer_should_error(self):
+
+    @dsl.pipeline(name='test-pipeline')
+    def my_pipeline():
+      from kfp.v2.dsl import importer, Artifact
+      importer(artifact_uri='dummy', artifact_class=Artifact)
+
+    with self.assertRaisesRegex(
+        NotImplementedError,
+        'dsl.importer is not supported for Kubeflow Pipelines open source yet.',
+    ):
+      kfp.compiler.Compiler().compile(
+          pipeline_func=my_pipeline, package_path='result.json')
+
+if __name__ == '__main__':
+  unittest.main()
