# Copyright 2020 kubeflow.org
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Repeat the line specified
      number of times", "inputs": [{"name": "line", "type": "String"}, {"default":
      "10", "name": "count", "optional": true, "type": "Integer"}], "name": "Repeat
      line", "outputs": [{"name": "output_text", "type": "String"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: repeat-line
spec:
  steps:
  - args:
    - --line
    - Hello
    - --count
    - '5000'
    - --output-text
    - $(workspaces.repeat-line.path)/repeat-line-output_text
    command:
    - python3
    - -u
    - -c
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \ndef repeat_line(line , output_text_path , count  = 10):\n    '''Repeat the\
      \ line specified number of times'''\n    with open(output_text_path, 'w') as\
      \ writer:\n        for i in range(count):\n            writer.write(line + '\\\
      n')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Repeat line',\
      \ description='Repeat the line specified number of times')\n_parser.add_argument(\"\
      --line\", dest=\"line\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--count\", dest=\"count\", type=int, required=False,\
      \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\", dest=\"\
      output_text_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
      _parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
      _output_paths\", [])\n\n_outputs = repeat_line(**_parsed_args)\n\n_output_serializers\
      \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: repeat-line
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Print text", "inputs":
      [{"name": "text"}], "name": "Print text"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-text
spec:
  steps:
  - args:
    - --text
    - $(workspaces.print-text.path)/repeat-line-output_text
    command:
    - python3
    - -u
    - -c
    - "def print_text(\n        text_path \n):  # The \"text\" input is untyped so\
      \ that any data can be printed\n    '''Print text'''\n    with open(text_path,\
      \ 'r') as reader:\n        for line in reader:\n            print(line, end='')\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Print text', description='Print\
      \ text')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = print_text(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: print-text
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "source", "type":
      "String"}], "name": "Split text lines", "outputs": [{"name": "odd_lines", "type":
      "String"}, {"name": "even_lines", "type": "String"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: split-text-lines
spec:
  stepTemplate:
    volumeMounts:
    - mountPath: /tmp/inputs/source
      name: source
  steps:
  - image: busybox
    name: copy-inputs
    script: '#!/bin/sh

      set -exo pipefail

      echo -n "one

      two

      three

      four

      five

      six

      seven

      eight

      nine

      ten" > /tmp/inputs/source/data

      '
  - args:
    - --source
    - /tmp/inputs/source/data
    - --odd-lines
    - $(workspaces.split-text-lines.path)/split-text-lines-odd_lines
    - --even-lines
    - $(workspaces.split-text-lines.path)/split-text-lines-even_lines
    command:
    - python3
    - -u
    - -c
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \ndef split_text_lines(source_path ,\n                     odd_lines_path ,\n\
      \                     even_lines_path ):\n    with open(source_path, 'r') as\
      \ reader:\n        with open(odd_lines_path, 'w') as odd_writer:\n         \
      \   with open(even_lines_path, 'w') as even_writer:\n                while True:\n\
      \                    line = reader.readline()\n                    if line ==\
      \ \"\":\n                        break\n                    odd_writer.write(line)\n\
      \                    line = reader.readline()\n                    if line ==\
      \ \"\":\n                        break\n                    even_writer.write(line)\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Split text lines',\
      \ description='')\n_parser.add_argument(\"--source\", dest=\"source_path\",\
      \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --odd-lines\", dest=\"odd_lines_path\", type=_make_parent_dirs_and_return_path,\
      \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--even-lines\"\
      , dest=\"even_lines_path\", type=_make_parent_dirs_and_return_path, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = split_text_lines(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  volumes:
  - emptyDir: {}
    name: source
  workspaces:
  - name: split-text-lines
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Print text", "inputs":
      [{"name": "text"}], "name": "Print text"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-text-2
spec:
  steps:
  - args:
    - --text
    - $(workspaces.print-text-2.path)/split-text-lines-odd_lines
    command:
    - python3
    - -u
    - -c
    - "def print_text(\n        text_path \n):  # The \"text\" input is untyped so\
      \ that any data can be printed\n    '''Print text'''\n    with open(text_path,\
      \ 'r') as reader:\n        for line in reader:\n            print(line, end='')\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Print text', description='Print\
      \ text')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = print_text(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: print-text-2
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Print text", "inputs":
      [{"name": "text"}], "name": "Print text"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-text-3
spec:
  steps:
  - args:
    - --text
    - $(workspaces.print-text-3.path)/split-text-lines-even_lines
    command:
    - python3
    - -u
    - -c
    - "def print_text(\n        text_path \n):  # The \"text\" input is untyped so\
      \ that any data can be printed\n    '''Print text'''\n    with open(text_path,\
      \ 'r') as reader:\n        for line in reader:\n            print(line, end='')\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Print text', description='Print\
      \ text')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = print_text(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: print-text-3
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"inputs": [{"default": "0", "name": "start",
      "optional": true, "type": "Integer"}, {"default": "10", "name": "count", "optional":
      true, "type": "Integer"}], "name": "Write numbers", "outputs": [{"name": "numbers",
      "type": "String"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: write-numbers
spec:
  steps:
  - args:
    - --count
    - '100000'
    - --numbers
    - $(workspaces.write-numbers.path)/write-numbers-numbers
    command:
    - python3
    - -u
    - -c
    - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n   \
      \ os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
      \ndef write_numbers(\n        numbers_path , start  = 0, count  = 10):\n   \
      \ with open(numbers_path, 'w') as writer:\n        for i in range(start, count):\n\
      \            writer.write(str(i) + '\\n')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Write\
      \ numbers', description='')\n_parser.add_argument(\"--start\", dest=\"start\"\
      , type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
      --count\", dest=\"count\", type=int, required=False, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"--numbers\", dest=\"numbers_path\", type=_make_parent_dirs_and_return_path,\
      \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = write_numbers(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: write-numbers
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Print text", "inputs":
      [{"name": "text"}], "name": "Print text"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-text-4
spec:
  steps:
  - args:
    - --text
    - $(workspaces.print-text-4.path)/write-numbers-numbers
    command:
    - python3
    - -u
    - -c
    - "def print_text(\n        text_path \n):  # The \"text\" input is untyped so\
      \ that any data can be printed\n    '''Print text'''\n    with open(text_path,\
      \ 'r') as reader:\n        for line in reader:\n            print(line, end='')\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Print text', description='Print\
      \ text')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = print_text(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: print-text-4
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "numbers", "type":
      "String"}], "name": "Sum numbers", "outputs": [{"name": "Output", "type": "Integer"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: sum-numbers
spec:
  steps:
  - args:
    - --numbers
    - $(workspaces.sum-numbers.path)/write-numbers-numbers
    - '----output-paths'
    - $(workspaces.sum-numbers.path)/sum-numbers-output
    command:
    - python3
    - -u
    - -c
    - "def sum_numbers(numbers_path )  :\n    sum = 0\n    with open(numbers_path,\
      \ 'r') as reader:\n        for line in reader:\n            sum = sum + int(line)\n\
      \    return sum\n\ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,\
      \ str):\n        return int_value\n    if not isinstance(int_value, int):\n\
      \        raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(str(int_value),\
      \ str(type(int_value))))\n    return str(int_value)\n\nimport argparse\n_parser\
      \ = argparse.ArgumentParser(prog='Sum numbers', description='')\n_parser.add_argument(\"\
      --numbers\", dest=\"numbers_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
      \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
      _output_paths\", [])\n\n_outputs = sum_numbers(**_parsed_args)\n\n_outputs =\
      \ [_outputs]\n\n_output_serializers = [\n    _serialize_int,\n\n]\n\nimport\
      \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: sum-numbers
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"description": "Print text", "inputs":
      [{"name": "text"}], "name": "Print text"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-text-5
spec:
  steps:
  - args:
    - --text
    - $(workspaces.print-text-5.path)/sum-numbers-output
    command:
    - python3
    - -u
    - -c
    - "def print_text(\n        text_path \n):  # The \"text\" input is untyped so\
      \ that any data can be printed\n    '''Print text'''\n    with open(text_path,\
      \ 'r') as reader:\n        for line in reader:\n            print(line, end='')\n\
      \nimport argparse\n_parser = argparse.ArgumentParser(prog='Print text', description='Print\
      \ text')\n_parser.add_argument(\"--text\", dest=\"text_path\", type=str, required=True,\
      \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n_output_files\
      \ = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = print_text(**_parsed_args)\n\
      \n_output_serializers = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
      \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
      \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
  workspaces:
  - name: print-text-5
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"name": "Gen params", "outputs": [{"name":
      "Output", "type": "Integer"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: gen-params
spec:
  results:
  - description: /tmp/outputs/Output/data
    name: output
  steps:
  - args:
    - '----output-paths'
    - $(results.output.path)
    command:
    - python3
    - -u
    - -c
    - "def gen_params()  :\n    import random\n    num = random.randint(0, 9)\n  \
      \  return num\n\ndef _serialize_int(int_value: int) -> str:\n    if isinstance(int_value,\
      \ str):\n        return int_value\n    if not isinstance(int_value, int):\n\
      \        raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(str(int_value),\
      \ str(type(int_value))))\n    return str(int_value)\n\nimport argparse\n_parser\
      \ = argparse.ArgumentParser(prog='Gen params', description='')\n_parser.add_argument(\"\
      ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = gen_params(**_parsed_args)\n\n_outputs = [_outputs]\n\n\
      _output_serializers = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx, output_file\
      \ in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
      \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
      \        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  annotations:
    pipelines.kubeflow.org/component_spec: '{"inputs": [{"name": "numbers_parm", "type":
      "Integer"}], "name": "Print params"}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: print-params
spec:
  params:
  - name: gen-params-output
  steps:
  - args:
    - --numbers-parm
    - $(inputs.params.gen-params-output)
    command:
    - python3
    - -u
    - -c
    - "def print_params(numbers_parm ):\n    print(\"The result number is: %d\" %\
      \ numbers_parm)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
      \ params', description='')\n_parser.add_argument(\"--numbers-parm\", dest=\"\
      numbers_parm\", type=int, required=True, default=argparse.SUPPRESS)\n_parsed_args\
      \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
      , [])\n\n_outputs = print_params(**_parsed_args)\n\n_output_serializers = [\n\
      \n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
      \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n  \
      \      pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
    image: tensorflow/tensorflow:1.13.2-py3
    name: main
---
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  annotations:
    pipelines.kubeflow.org/pipeline_spec: '{"name": "File passing pipelines"}'
    sidecar.istio.io/inject: 'false'
  name: file-passing-pipelines
spec:
  tasks:
  - name: repeat-line
    taskRef:
      name: repeat-line
    workspaces:
    - name: repeat-line
      workspace: file-passing-pipelines
  - name: print-text
    runAfter:
    - repeat-line
    taskRef:
      name: print-text
    workspaces:
    - name: print-text
      workspace: file-passing-pipelines
  - name: split-text-lines
    taskRef:
      name: split-text-lines
    workspaces:
    - name: split-text-lines
      workspace: file-passing-pipelines
  - name: print-text-2
    runAfter:
    - split-text-lines
    taskRef:
      name: print-text-2
    workspaces:
    - name: print-text-2
      workspace: file-passing-pipelines
  - name: print-text-3
    runAfter:
    - split-text-lines
    taskRef:
      name: print-text-3
    workspaces:
    - name: print-text-3
      workspace: file-passing-pipelines
  - name: write-numbers
    taskRef:
      name: write-numbers
    workspaces:
    - name: write-numbers
      workspace: file-passing-pipelines
  - name: print-text-4
    runAfter:
    - write-numbers
    taskRef:
      name: print-text-4
    workspaces:
    - name: print-text-4
      workspace: file-passing-pipelines
  - name: sum-numbers
    runAfter:
    - write-numbers
    taskRef:
      name: sum-numbers
    workspaces:
    - name: sum-numbers
      workspace: file-passing-pipelines
  - name: print-text-5
    runAfter:
    - sum-numbers
    taskRef:
      name: print-text-5
    workspaces:
    - name: print-text-5
      workspace: file-passing-pipelines
  - name: gen-params
    taskRef:
      name: gen-params
  - name: print-params
    params:
    - name: gen-params-output
      value: $(tasks.gen-params.results.output)
    taskRef:
      name: print-params
  workspaces:
  - name: file-passing-pipelines
---
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  annotations:
    tekton.dev/input_artifacts: '{}'
    tekton.dev/output_artifacts: '{}'
  name: file-passing-pipelines-run
spec:
  pipelineRef:
    name: file-passing-pipelines
  workspaces:
  - name: file-passing-pipelines
    persistentVolumeClaim:
      claimName: file-passing-pipelines-run
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: file-passing-pipelines-run
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
  volumeMode: Filesystem
