# Copyright 2020 kubeflow.org
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  annotations:
    pipelines.kubeflow.org/pipeline_spec: '{"name": "File passing pipelines"}'
    sidecar.istio.io/inject: 'false'
    tekton.dev/input_artifacts: '{"print-params": [{"name": "gen-params-output", "parent_task":
      "gen-params"}], "print-text": [{"name": "repeat-line-output_text", "parent_task":
      "repeat-line"}], "print-text-2": [{"name": "split-text-lines-odd_lines", "parent_task":
      "split-text-lines"}], "print-text-3": [{"name": "split-text-lines-even_lines",
      "parent_task": "split-text-lines"}], "print-text-4": [{"name": "write-numbers-numbers",
      "parent_task": "write-numbers"}], "print-text-5": [{"name": "sum-numbers-output",
      "parent_task": "sum-numbers"}], "sum-numbers": [{"name": "write-numbers-numbers",
      "parent_task": "write-numbers"}]}'
    tekton.dev/output_artifacts: '{"gen-params": [{"name": "gen-params-output", "path":
      "/tmp/outputs/Output/data"}], "repeat-line": [{"name": "repeat-line-output_text",
      "path": "/tmp/outputs/output_text/data"}], "split-text-lines": [{"name": "split-text-lines-even_lines",
      "path": "/tmp/outputs/even_lines/data"}, {"name": "split-text-lines-odd_lines",
      "path": "/tmp/outputs/odd_lines/data"}], "sum-numbers": [{"name": "sum-numbers-output",
      "path": "/tmp/outputs/Output/data"}], "write-numbers": [{"name": "write-numbers-numbers",
      "path": "/tmp/outputs/numbers/data"}]}'
  labels:
    pipelines.kubeflow.org/pipeline-sdk-type: kfp
  name: file-passing-pipelines
spec:
  pipelineSpec:
    tasks:
    - name: repeat-line
      taskSpec:
        steps:
        - args:
          - --line
          - Hello
          - --count
          - '5000'
          - --output-text
          - $(workspaces.repeat-line.path)/repeat-line-output_text
          command:
          - python3
          - -u
          - -c
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef repeat_line(line , output_text_path , count  = 10):\n\
            \    '''Repeat the line specified number of times'''\n    with open(output_text_path,\
            \ 'w') as writer:\n        for i in range(count):\n            writer.write(line\
            \ + '\\n')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Repeat\
            \ line', description='Repeat the line specified number of times')\n_parser.add_argument(\"\
            --line\", dest=\"line\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--count\", dest=\"count\", type=int, required=False,\
            \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--output-text\",\
            \ dest=\"output_text_path\", type=_make_parent_dirs_and_return_path, required=True,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ repeat_line(**_parsed_args)\n\n_output_serializers = [\n\n]\n\nimport\
            \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n  \
            \      os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
            \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage http://minio-service.$NAMESPACE:9000 $AWS_ACCESS_KEY_ID
            $AWS_SECRET_ACCESS_KEY

            tar -cvzf output_text.tgz $(workspaces.repeat-line.path)/repeat-line-output_text

            mc cp output_text.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/output_text.tgz

            '
        workspaces:
        - name: repeat-line
      workspaces:
      - name: repeat-line
        workspace: file-passing-pipelines
    - name: print-text
      runAfter:
      - repeat-line
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text.path)/repeat-line-output_text
          command:
          - python3
          - -u
          - -c
          - "def print_text(\n        text_path \n):  # The \"text\" input is untyped\
            \ so that any data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end='')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text
      workspaces:
      - name: print-text
        workspace: file-passing-pipelines
    - name: split-text-lines
      taskSpec:
        stepTemplate:
          volumeMounts:
          - mountPath: /tmp/inputs/source
            name: source
        steps:
        - image: busybox
          name: copy-inputs
          script: '#!/bin/sh

            set -exo pipefail

            echo -n "one

            two

            three

            four

            five

            six

            seven

            eight

            nine

            ten" > /tmp/inputs/source/data

            '
        - args:
          - --source
          - /tmp/inputs/source/data
          - --odd-lines
          - $(workspaces.split-text-lines.path)/split-text-lines-odd_lines
          - --even-lines
          - $(workspaces.split-text-lines.path)/split-text-lines-even_lines
          command:
          - python3
          - -u
          - -c
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef split_text_lines(source_path ,\n                  \
            \   odd_lines_path ,\n                     even_lines_path ):\n    with\
            \ open(source_path, 'r') as reader:\n        with open(odd_lines_path,\
            \ 'w') as odd_writer:\n            with open(even_lines_path, 'w') as\
            \ even_writer:\n                while True:\n                    line\
            \ = reader.readline()\n                    if line == \"\":\n        \
            \                break\n                    odd_writer.write(line)\n \
            \                   line = reader.readline()\n                    if line\
            \ == \"\":\n                        break\n                    even_writer.write(line)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Split text\
            \ lines', description='')\n_parser.add_argument(\"--source\", dest=\"\
            source_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --odd-lines\", dest=\"odd_lines_path\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--even-lines\"\
            , dest=\"even_lines_path\", type=_make_parent_dirs_and_return_path, required=True,\
            \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ split_text_lines(**_parsed_args)\n\n_output_serializers = [\n\n]\n\n\
            import os\nfor idx, output_file in enumerate(_output_files):\n    try:\n\
            \        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
            \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage http://minio-service.$NAMESPACE:9000 $AWS_ACCESS_KEY_ID
            $AWS_SECRET_ACCESS_KEY

            tar -cvzf even_lines.tgz $(workspaces.split-text-lines.path)/split-text-lines-even_lines

            mc cp even_lines.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/even_lines.tgz

            tar -cvzf odd_lines.tgz $(workspaces.split-text-lines.path)/split-text-lines-odd_lines

            mc cp odd_lines.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/odd_lines.tgz

            '
        volumes:
        - emptyDir: {}
          name: source
        workspaces:
        - name: split-text-lines
      workspaces:
      - name: split-text-lines
        workspace: file-passing-pipelines
    - name: print-text-2
      runAfter:
      - split-text-lines
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text-2.path)/split-text-lines-odd_lines
          command:
          - python3
          - -u
          - -c
          - "def print_text(\n        text_path \n):  # The \"text\" input is untyped\
            \ so that any data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end='')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text-2
      workspaces:
      - name: print-text-2
        workspace: file-passing-pipelines
    - name: print-text-3
      runAfter:
      - split-text-lines
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text-3.path)/split-text-lines-even_lines
          command:
          - python3
          - -u
          - -c
          - "def print_text(\n        text_path \n):  # The \"text\" input is untyped\
            \ so that any data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end='')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text-3
      workspaces:
      - name: print-text-3
        workspace: file-passing-pipelines
    - name: write-numbers
      taskSpec:
        steps:
        - args:
          - --count
          - '100000'
          - --numbers
          - $(workspaces.write-numbers.path)/write-numbers-numbers
          command:
          - python3
          - -u
          - -c
          - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n\
            \    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return\
            \ file_path\n\ndef write_numbers(\n        numbers_path , start  = 0,\
            \ count  = 10):\n    with open(numbers_path, 'w') as writer:\n       \
            \ for i in range(start, count):\n            writer.write(str(i) + '\\\
            n')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Write\
            \ numbers', description='')\n_parser.add_argument(\"--start\", dest=\"\
            start\", type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --count\", dest=\"count\", type=int, required=False, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"--numbers\", dest=\"numbers_path\", type=_make_parent_dirs_and_return_path,\
            \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ write_numbers(**_parsed_args)\n\n_output_serializers = [\n\n]\n\nimport\
            \ os\nfor idx, output_file in enumerate(_output_files):\n    try:\n  \
            \      os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
            \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage http://minio-service.$NAMESPACE:9000 $AWS_ACCESS_KEY_ID
            $AWS_SECRET_ACCESS_KEY

            tar -cvzf numbers.tgz $(workspaces.write-numbers.path)/write-numbers-numbers

            mc cp numbers.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/numbers.tgz

            '
        workspaces:
        - name: write-numbers
      workspaces:
      - name: write-numbers
        workspace: file-passing-pipelines
    - name: print-text-4
      runAfter:
      - write-numbers
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text-4.path)/write-numbers-numbers
          command:
          - python3
          - -u
          - -c
          - "def print_text(\n        text_path \n):  # The \"text\" input is untyped\
            \ so that any data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end='')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text-4
      workspaces:
      - name: print-text-4
        workspace: file-passing-pipelines
    - name: sum-numbers
      runAfter:
      - write-numbers
      taskSpec:
        steps:
        - args:
          - --numbers
          - $(workspaces.sum-numbers.path)/write-numbers-numbers
          - '----output-paths'
          - $(workspaces.sum-numbers.path)/sum-numbers-output
          command:
          - python3
          - -u
          - -c
          - "def sum_numbers(numbers_path )  :\n    sum = 0\n    with open(numbers_path,\
            \ 'r') as reader:\n        for line in reader:\n            sum = sum\
            \ + int(line)\n    return sum\n\ndef _serialize_int(int_value: int) ->\
            \ str:\n    if isinstance(int_value, str):\n        return int_value\n\
            \    if not isinstance(int_value, int):\n        raise TypeError('Value\
            \ \"{}\" has type \"{}\" instead of int.'.format(str(int_value), str(type(int_value))))\n\
            \    return str(int_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Sum\
            \ numbers', description='')\n_parser.add_argument(\"--numbers\", dest=\"\
            numbers_path\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str,\
            \ nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files =\
            \ _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = sum_numbers(**_parsed_args)\n\
            \n_outputs = [_outputs]\n\n_output_serializers = [\n    _serialize_int,\n\
            \n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage http://minio-service.$NAMESPACE:9000 $AWS_ACCESS_KEY_ID
            $AWS_SECRET_ACCESS_KEY

            tar -cvzf output.tgz $(workspaces.sum-numbers.path)/sum-numbers-output

            mc cp output.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/output.tgz

            '
        workspaces:
        - name: sum-numbers
      workspaces:
      - name: sum-numbers
        workspace: file-passing-pipelines
    - name: print-text-5
      runAfter:
      - sum-numbers
      taskSpec:
        steps:
        - args:
          - --text
          - $(workspaces.print-text-5.path)/sum-numbers-output
          command:
          - python3
          - -u
          - -c
          - "def print_text(\n        text_path \n):  # The \"text\" input is untyped\
            \ so that any data can be printed\n    '''Print text'''\n    with open(text_path,\
            \ 'r') as reader:\n        for line in reader:\n            print(line,\
            \ end='')\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ text', description='Print text')\n_parser.add_argument(\"--text\", dest=\"\
            text_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args\
            \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
            , [])\n\n_outputs = print_text(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        workspaces:
        - name: print-text-5
      workspaces:
      - name: print-text-5
        workspace: file-passing-pipelines
    - name: gen-params
      taskSpec:
        results:
        - description: /tmp/outputs/Output/data
          name: output
        steps:
        - args:
          - '----output-paths'
          - $(results.output.path)
          command:
          - python3
          - -u
          - -c
          - "def gen_params()  :\n    import random\n    num = random.randint(0, 9)\n\
            \    return num\n\ndef _serialize_int(int_value: int) -> str:\n    if\
            \ isinstance(int_value, str):\n        return int_value\n    if not isinstance(int_value,\
            \ int):\n        raise TypeError('Value \"{}\" has type \"{}\" instead\
            \ of int.'.format(str(int_value), str(type(int_value))))\n    return str(int_value)\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Gen params',\
            \ description='')\n_parser.add_argument(\"----output-paths\", dest=\"\
            _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
            _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs =\
            \ gen_params(**_parsed_args)\n\n_outputs = [_outputs]\n\n_output_serializers\
            \ = [\n    _serialize_int,\n\n]\n\nimport os\nfor idx, output_file in\
            \ enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
            \    except OSError:\n        pass\n    with open(output_file, 'w') as\
            \ f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
        - env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineRun']
          - name: PIPELINETASK
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['tekton.dev/pipelineTask']
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: accesskey
                name: mlpipeline-minio-artifact
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: secretkey
                name: mlpipeline-minio-artifact
          image: minio/mc
          name: copy-artifacts
          script: '#!/usr/bin/env sh

            mc config host add storage http://minio-service.$NAMESPACE:9000 $AWS_ACCESS_KEY_ID
            $AWS_SECRET_ACCESS_KEY

            tar -cvzf output.tgz $(results.output.path)

            mc cp output.tgz storage/mlpipeline/artifacts/$PIPELINERUN/$PIPELINETASK/output.tgz

            '
    - name: print-params
      params:
      - name: gen-params-output
        value: $(tasks.gen-params.results.output)
      taskSpec:
        params:
        - name: gen-params-output
        steps:
        - args:
          - --numbers-parm
          - $(inputs.params.gen-params-output)
          command:
          - python3
          - -u
          - -c
          - "def print_params(numbers_parm ):\n    print(\"The result number is: %d\"\
            \ % numbers_parm)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Print\
            \ params', description='')\n_parser.add_argument(\"--numbers-parm\", dest=\"\
            numbers_parm\", type=int, required=True, default=argparse.SUPPRESS)\n\
            _parsed_args = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"\
            _output_paths\", [])\n\n_outputs = print_params(**_parsed_args)\n\n_output_serializers\
            \ = [\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
            \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except\
            \ OSError:\n        pass\n    with open(output_file, 'w') as f:\n    \
            \    f.write(_output_serializers[idx](_outputs[idx]))\n"
          image: tensorflow/tensorflow:1.13.2-py3
          name: main
    workspaces:
    - name: file-passing-pipelines
  workspaces:
  - name: file-passing-pipelines
    persistentVolumeClaim:
      claimName: file-passing-pipelines
