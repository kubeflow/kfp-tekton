# Copyright 2023 kubeflow.org
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: artifact-passing-pipeline
  annotations:
    tekton.dev/output_artifacts: '{"metadata-and-metrics": [{"key": "artifacts/$PIPELINERUN/metadata-and-metrics/mlpipeline-ui-metadata.tgz",
      "name": "mlpipeline-ui-metadata", "path": "/tmp/outputs/mlpipeline_ui_metadata/data"},
      {"key": "artifacts/$PIPELINERUN/metadata-and-metrics/mlpipeline-metrics.tgz",
      "name": "mlpipeline-metrics", "path": "/tmp/outputs/mlpipeline_metrics/data"}],
      "processor": [{"key": "artifacts/$PIPELINERUN/processor/Output-1.tgz", "name":
      "processor-Output-1", "path": "/tmp/outputs/Output_1/data"}, {"key": "artifacts/$PIPELINERUN/processor/Output-2.tgz",
      "name": "processor-Output-2", "path": "/tmp/outputs/Output_2/data"}], "producer":
      [{"key": "artifacts/$PIPELINERUN/producer/Output-1.tgz", "name": "producer-Output-1",
      "path": "/tmp/outputs/Output_1/data"}, {"key": "artifacts/$PIPELINERUN/producer/Output-2.tgz",
      "name": "producer-Output-2", "path": "/tmp/outputs/Output_2/data"}]}'
    tekton.dev/input_artifacts: '{"consumer": [{"name": "processor-Output-1", "parent_task":
      "processor"}, {"name": "processor-Output-2", "parent_task": "processor"}], "processor":
      [{"name": "producer-Output-1", "parent_task": "producer"}, {"name": "producer-Output-2",
      "parent_task": "producer"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"consumer": [], "metadata-and-metrics": [["mlpipeline-ui-metadata",
      "/tmp/outputs/mlpipeline_ui_metadata/data"], ["mlpipeline-metrics", "/tmp/outputs/mlpipeline_metrics/data"]],
      "processor": [["Output-1", "$(results.Output-1.path)"], ["Output-2", "$(workspaces.processor.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2"]],
      "producer": [["Output-1", "$(results.Output-1.path)"], ["Output-2", "$(workspaces.producer.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2"]]}'
    sidecar.istio.io/inject: "false"
    tekton.dev/template: ''
    pipelines.kubeflow.org/big_data_passing_format: $(workspaces.$TASK_NAME.path)/artifacts/$ORIG_PR_NAME/$TASKRUN_NAME/$TASK_PARAM_NAME
    pipelines.kubeflow.org/pipeline_spec: '{"name": "Artifact passing pipeline"}'
  labels:
    pipelines.kubeflow.org/pipelinename: ''
    pipelines.kubeflow.org/generation: ''
spec:
  pipelineSpec:
    tasks:
    - name: producer
      taskSpec:
        steps:
        - name: main
          args:
          - $(results.Output-1.path)
          - $(workspaces.producer.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2
          command:
          - sh
          - -c
          - |
            mkdir -p "$(dirname "$0")"
            mkdir -p "$(dirname "$1")"
            echo "Data 1" > $0
            echo "Data 2" > $1
          image: alpine
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.producer.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2 $(results.Output-2.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        results:
        - name: Output-1
          type: string
          description: /tmp/outputs/Output_1/data
        - name: Output-2
          type: string
          description: /tmp/outputs/Output_2/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Producer", "outputs":
              [{"name": "Output 1"}, {"name": "Output 2"}], "version": "Producer@sha256=f8c0448df5bf22241b635044bce2463869c10f9b450e979b7da029a931486367"}'
        workspaces:
        - name: producer
      workspaces:
      - name: producer
        workspace: artifact-passing-pipeline
    - name: processor
      params:
      - name: producer-Output-1
        value: $(tasks.producer.results.Output-1)
      - name: producer-trname
        value: $(tasks.producer.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - $(inputs.params.producer-Output-1)
          - $(workspaces.processor.path)/artifacts/$ORIG_PR_NAME/$(params.producer-trname)/Output-2
          - $(results.Output-1.path)
          - $(workspaces.processor.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2
          command:
          - sh
          - -c
          - |
            mkdir -p "$(dirname "$2")"
            mkdir -p "$(dirname "$3")"
            echo "$0" > "$2"
            cp "$1" "$3"
          image: alpine
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        - image: busybox
          name: output-taskrun-name
          command:
          - sh
          - -ec
          - echo -n "$(context.taskRun.name)" > "$(results.taskrun-name.path)"
        - image: busybox
          name: copy-results-artifacts
          command:
          - sh
          - -ec
          - |
            set -exo pipefail
            TOTAL_SIZE=0
            copy_artifact() {
            if [ -d "$1" ]; then
              tar -czvf "$1".tar.gz "$1"
              SUFFIX=".tar.gz"
            fi
            ARTIFACT_SIZE=`wc -c "$1"${SUFFIX} | awk '{print $1}'`
            TOTAL_SIZE=$( expr $TOTAL_SIZE + $ARTIFACT_SIZE)
            touch "$2"
            if [[ $TOTAL_SIZE -lt 3072 ]]; then
              if [ -d "$1" ]; then
                tar -tzf "$1".tar.gz > "$2"
              elif ! awk "/[^[:print:]]/{f=1} END{exit !f}" "$1"; then
                cp "$1" "$2"
              fi
            fi
            }
            copy_artifact $(workspaces.processor.path)/artifacts/$ORIG_PR_NAME/$(context.taskRun.name)/Output-2 $(results.Output-2.path)
          onError: continue
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: producer-Output-1
        - name: producer-trname
        results:
        - name: Output-1
          type: string
          description: /tmp/outputs/Output_1/data
        - name: Output-2
          type: string
          description: /tmp/outputs/Output_2/data
        - name: taskrun-name
          type: string
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Processor", "outputs":
              [{"name": "Output 1"}, {"name": "Output 2"}], "version": "Processor@sha256=dc713ae50cd79f58567512103e2e889ad7505f87770c6681731f94319615f373"}'
        workspaces:
        - name: processor
      workspaces:
      - name: processor
        workspace: artifact-passing-pipeline
      runAfter:
      - producer
    - name: consumer
      params:
      - name: processor-Output-1
        value: $(tasks.processor.results.Output-1)
      - name: processor-trname
        value: $(tasks.processor.results.taskrun-name)
      taskSpec:
        steps:
        - name: main
          args:
          - $(inputs.params.processor-Output-1)
          - $(workspaces.consumer.path)/artifacts/$ORIG_PR_NAME/$(params.processor-trname)/Output-2
          command:
          - sh
          - -c
          - |
            echo "Input parameter = $0"
            echo "Input artifact = " && cat "$1"
          image: alpine
          env:
          - name: ORIG_PR_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['custom.tekton.dev/originalPipelineRun']
        params:
        - name: processor-Output-1
        - name: processor-trname
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Consumer", "outputs":
              [], "version": "Consumer@sha256=ae9cf1975753ed1382e04db32bd2440d61a9cbfb893ec327ff15199589311701"}'
        workspaces:
        - name: consumer
      workspaces:
      - name: consumer
        workspace: artifact-passing-pipeline
      runAfter:
      - processor
    - name: metadata-and-metrics
      taskSpec:
        steps:
        - name: main
          args:
          - '----output-paths'
          - /tmp/outputs/mlpipeline_ui_metadata/data
          - /tmp/outputs/mlpipeline_metrics/data
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def metadata_and_metrics():
                metadata = {
                    "outputs": [{
                        "storage": "inline",
                        "source": "*this should be bold*",
                        "type": "markdown"
                    }]
                }
                metrics = {
                    "metrics": [
                        {
                            "name": "train-accuracy",
                            "numberValue": 0.9,
                        },
                        {
                            "name": "test-accuracy",
                            "numberValue": 0.7,
                        },
                    ]
                }
                from collections import namedtuple
                import json

                return namedtuple("output",
                                  ["mlpipeline_ui_metadata", "mlpipeline_metrics"])(
                                      json.dumps(metadata), json.dumps(metrics))

            import argparse
            _parser = argparse.ArgumentParser(prog='Metadata and metrics', description='')
            _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
            _parsed_args = vars(_parser.parse_args())
            _output_files = _parsed_args.pop("_output_paths", [])

            _outputs = metadata_and_metrics(**_parsed_args)

            _output_serializers = [
                str,
                str,

            ]

            import os
            for idx, output_file in enumerate(_output_files):
                try:
                    os.makedirs(os.path.dirname(output_file))
                except OSError:
                    pass
                with open(output_file, 'w') as f:
                    f.write(_output_serializers[idx](_outputs[idx]))
          image: python:3.7
        stepTemplate:
          volumeMounts:
          - name: mlpipeline-ui-metadata
            mountPath: /tmp/outputs/mlpipeline_ui_metadata
          - name: mlpipeline-metrics
            mountPath: /tmp/outputs/mlpipeline_metrics
        volumes:
        - name: mlpipeline-ui-metadata
          emptyDir: {}
        - name: mlpipeline-metrics
          emptyDir: {}
        metadata:
          labels:
            pipelines.kubeflow.org/cache_enabled: "true"
          annotations:
            pipelines.kubeflow.org/component_spec_digest: '{"name": "Metadata and
              metrics", "outputs": [{"name": "mlpipeline_ui_metadata", "type": "UI_metadata"},
              {"name": "mlpipeline_metrics", "type": "Metrics"}], "version": "Metadata
              and metrics@sha256=457911fc37b191922b25d050fefef6a3d87e0351f35d0838a053245bd4095e8c"}'
    workspaces:
    - name: artifact-passing-pipeline
  workspaces:
  - name: artifact-passing-pipeline
    persistentVolumeClaim:
      claimName: data-volume
    subPath: artifact_data/
